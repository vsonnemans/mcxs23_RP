[
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Research question",
    "text": "Research question\nCan Bayesian VARs and alternative data help better estimate the future state of Personal Consumption Expenditures in the US?"
  },
  {
    "objectID": "index.html#objective-and-motivation",
    "href": "index.html#objective-and-motivation",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Objective and Motivation",
    "text": "Objective and Motivation\nAs it drives around 50% of the US GDP, Personal Consumption Expenditures (PCE) is a leading indicator to gauge the economic health of a country. There is thus a high incentive to improve the accuracy of its predictions. This has encouraged researchers to investigate big data as alternative sources. For instance, Schimdt and Vosen (2009) use search query time-series provided by Google Trends to forecast consumption. Esteves (2009), Aprigliano, et al. (2019), Galbraith and Tkacz (2013), Carlsen and Storgaard (2010) analyze electronic payments to predict consumption as these can track a large percentage of spending activities. Ellingsen, et al. (2021) demonstrate that news media data capture information about consumption that hard economic indicators do not. Gil et al. (2018) investigate the potential of the Economic Policy Uncertainty index derived from news data and developed by Baker et al. (2016) to predict consumption.\nAccording to Professor Tomasz Wozniak from the University of Melbourne, forecasting with Bayesian VARs often leads to more precise forecasts than when using the frequentist approach to forecasting because of the effect of the priors. Despite the benefits of Bayesian VARs, there is nearly no research on the combination of Bayesian VARs and alternative data. Existing articles either investigate the use of Bayesian estimation models or of alternative data to forecast indicators but do not consider both together.\nThis paper will compute and compare the forecasts of PCE in the US from Bayesian VARs and several extensions applied on traditional macroeconomic variables computed by statistical offices and alternative variables such as Google Trends. This research project contributes to the literature by studying PCE, an indicator that has to date received scant attention from the Bayesian VARs literature. Moreover, it proposes the first investigation of the combination of Bayesian VARs with alternative data to forecast PCE."
  },
  {
    "objectID": "index.html#preliminary-data-analysis",
    "href": "index.html#preliminary-data-analysis",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Preliminary data analysis",
    "text": "Preliminary data analysis\n\nAugmented Dickey-Fuller test for unit roots of traditional variables\n\nTable 1. ADF results for traditional variables\n\n\n\n\n  \n\n\n\n\n\nTable 2. ADF results for first difference of traditional variables\n\n\n\n\n  \n\n\n\n\n\nTable 3. ADF results for the second difference of Home price index\n\n\n\n\n  \n\n\n\n\n\n\nAugmented Dickey-Fuller test for unit roots of alternative variables\n\nTable 4. ADF results for the first difference of GT for PCE of durable goods\n\n\n\n\n  \n\n\n\n\n\nTable 5. ADF results for the first difference of GT for PCE of non-durable goods\n\n\n\n\n  \n\n\n\n\n\nTable 6. ADF results for the first difference of GT for PCE of services\n\n\n\n\n  \n\n\n\n\n\n\nPrincipal Component Analysis on Google Trends data\nFollowing the methodology in Schimdt and Vosen (2009), we will conduct a Principal Component Analysis on all Google Trends series to reduce the dimensionality of our dataset. The PCA is applied 3 times: one for the GT series related to PCE of durable goods, one for the GT series related to PCE of non-durable goods and one for the GT series related to PCE of services. We proceed as following:\n\nNormalising the data\n\n\nComputing the correlation matrices\n\n\n\n\n\n\n\n\n\n\n\n\nThe correlation matrices demonstrate strong relations between most of the variables. We therefore conclude that a PCA is appropriate.\n\nApplying PCA on the correlation matrices\n\nFor PCE of durable goods, the summary table of PCA results show that the cumulative proportion of the first 2 components (Comp.1 and Comp.2) explain 88.05% of the total variance. The first component accounts for 72.32% of the variance and component 2 for only 15.73%. Therefore we make the choice to only use component 1.\n\nTable 7. PCA results for GT for PCE of durable goods\n\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5    PC6    PC7\nStandard deviation     3.9887 1.8604 0.82472 0.74564 0.59551 0.4597 0.3840\nProportion of Variance 0.7232 0.1573 0.03092 0.02527 0.01612 0.0096 0.0067\nCumulative Proportion  0.7232 0.8805 0.91142 0.93669 0.95281 0.9624 0.9691\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.37037 0.34255 0.30581 0.26989 0.22162 0.20700 0.18551\nProportion of Variance 0.00624 0.00533 0.00425 0.00331 0.00223 0.00195 0.00156\nCumulative Proportion  0.97535 0.98069 0.98494 0.98825 0.99048 0.99243 0.99399\n                          PC15    PC16   PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.16288 0.14970 0.1324 0.12906 0.12358 0.11270 0.10567\nProportion of Variance 0.00121 0.00102 0.0008 0.00076 0.00069 0.00058 0.00051\nCumulative Proportion  0.99520 0.99622 0.9970 0.99777 0.99847 0.99904 0.99955\n                          PC22\nStandard deviation     0.09945\nProportion of Variance 0.00045\nCumulative Proportion  1.00000\n\n\nFor PCE of non-durable goods, we also take the first and second components as the first one only accounts for 47%. Together the two components explain 80.33% of the total variance.\n\n\nTable 8. PCA results for GT for PCE of non-durable goods\n\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     3.0670 2.5806 1.10705 0.83951 0.70666 0.58067 0.48188\nProportion of Variance 0.4703 0.3330 0.06128 0.03524 0.02497 0.01686 0.01161\nCumulative Proportion  0.4703 0.8033 0.86457 0.89981 0.92478 0.94164 0.95325\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.42032 0.37535 0.34214 0.32980 0.28856 0.26265 0.24208\nProportion of Variance 0.00883 0.00704 0.00585 0.00544 0.00416 0.00345 0.00293\nCumulative Proportion  0.96208 0.96912 0.97498 0.98042 0.98458 0.98803 0.99096\n                          PC15   PC16    PC17    PC18    PC19    PC20\nStandard deviation     0.21633 0.2100 0.17767 0.15568 0.13154 0.12975\nProportion of Variance 0.00234 0.0022 0.00158 0.00121 0.00087 0.00084\nCumulative Proportion  0.99330 0.9955 0.99708 0.99829 0.99916 1.00000\n\n\nFor PCE of services, we also take only the first component as it accounts for 62.5%.\n\n\nTable 9. PCA results for GT for PCE of services\n\n\nImportance of components:\n                         PC1    PC2     PC3     PC4     PC5    PC6     PC7\nStandard deviation     3.873 2.2131 1.04542 0.89470 0.73197 0.5899 0.54149\nProportion of Variance 0.625 0.2041 0.04554 0.03335 0.02232 0.0145 0.01222\nCumulative Proportion  0.625 0.8290 0.87459 0.90795 0.93027 0.9448 0.95699\n                           PC8     PC9    PC10    PC11   PC12    PC13   PC14\nStandard deviation     0.44220 0.37394 0.35055 0.34079 0.3019 0.28800 0.2593\nProportion of Variance 0.00815 0.00583 0.00512 0.00484 0.0038 0.00346 0.0028\nCumulative Proportion  0.96514 0.97096 0.97608 0.98092 0.9847 0.98818 0.9910\n                          PC15    PC16   PC17    PC18   PC19    PC20    PC21\nStandard deviation     0.23067 0.20023 0.1900 0.17216 0.1389 0.12797 0.11670\nProportion of Variance 0.00222 0.00167 0.0015 0.00123 0.0008 0.00068 0.00057\nCumulative Proportion  0.99319 0.99486 0.9964 0.99760 0.9984 0.99909 0.99966\n                          PC22     PC23      PC24\nStandard deviation     0.09082 1.02e-15 5.678e-17\nProportion of Variance 0.00034 0.00e+00 0.000e+00\nCumulative Proportion  1.00000 1.00e+00 1.000e+00\n\n\nAfter conducting the PCA analysis and keeping the first principal component which explains a considerable proportion of the variances, we obtain three transformed variables names GT_dur, GT_ndur and GT_sv."
  },
  {
    "objectID": "index.html#the-baseline-model",
    "href": "index.html#the-baseline-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "The Baseline Model",
    "text": "The Baseline Model\nThe model used for the forecasting experiment is a VAR(p) model:\n\\[\\begin{aligned}\n& y_t  =\\mu_0+A_1 y_{t-1}+\\cdots+A_p y_{t-p}+\\epsilon_t \\\\ & \\epsilon_t \\mid  Y_{t-1}  \\sim i i d \\mathcal{N}_N\\left(\\mathbf{0}_N, \\Sigma\\right)\n\\end{aligned}\\]\nWhere \\(N=11\\) and \\(y_t\\) is the vector of 11 variables:\nThe model can also be written in matrix notation:\n\\[\\begin{aligned}\nY & =X A+E \\\\E \\mid X & \\sim \\mathcal{M N} _{T \\times N}\\left(\\mathbf{0}_{T \\times N}, \\Sigma, I_T\\right)\n\\end{aligned}\\]\nWhere \\(Y\\) is a \\(T\\times11\\) matrix, \\(X\\) is a \\(T\\times(1+(11\\times p))\\), \\(A\\) is a \\((1+(11\\times p))\\times 11\\) matrix that contains the relationships between the variables and \\(E\\) is a \\(T\\times11\\). \\(p\\) is 4 and we have 230 observations of monthly data.\n\nThe Likelihood function\n\\[\n\\begin{aligned}\nY \\mid X,A,\\Sigma & \\sim \\mathcal{M N} _{T \\times N}\\left(XA, \\Sigma, I_T\\right)\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\nL(A, \\Sigma \\mid Y, X) & \\propto det(\\Sigma)^{-\\frac{T}{2}} \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X A)^{\\prime}(Y-X A)\\right]\\right\\} \\\\\n&= det (\\Sigma)^{-\\frac{T}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})\\right]\\right\\}\n\\end{aligned}\n\\]\n\n\nThe prior distributions\n\\[\n\\begin{aligned}\n&A \\mid \\Sigma \\sim  \\mathcal{M N} _{T \\times N}(\\underline{A}, \\Sigma, \\underline{V}) \\\\\n&\\Sigma \\sim \\mathcal{IW}_{N}(\\underline{S}, \\underline{\\nu})\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\np(A, \\Sigma) \\propto & \\operatorname{det}(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1} \\underline{S}\\right]\\right\\}\n\\end{aligned}\n\\]\nAs we follow the Minnesota prior specifications, we set in the code:\n\\[\n\\underline{A}=[0_{N\\times 1}\\space\\space\\space I_N \\space\\space\\space 0_{N\\times (p-1)N} ]'\n\\\\ \\underline{V}=diag([\\kappa _2 \\space\\space\\kappa_1(p^{-2}\\otimes 1'_N)]\n\\\\ \\underline{\\nu}=N+1\n\\]\n\n\nThe posterior distribution\n\nThe Derivations of the posterior distribution\n\\[\n\\begin{aligned}\np(A, \\Sigma \\mid Y, X) \\propto & \\operatorname{det}(\\Sigma)^{-\\frac{T}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})\\right]\\right\\} \\\\\n& \\times \\operatorname{det}(\\Sigma)^{-\\frac{N+K+\\underline{+}+1}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1} \\underline{S}\\right]\\right\\},\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n& p(A, \\Sigma \\mid Y, X) \\propto \\operatorname{det}(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma ^ { - 1 } \\left[(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})+(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right.\\right.\\right. \\\\\n& \\left.\\left.\\left.+(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})+\\underline{S}\\right]\\right]\\right\\}\n\\end{aligned}\n\\]\nWe can now complete the squares.\n\\[\\begin{aligned}\n&\n\\color{blue}{(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})}\n\\color{green}{\n+(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})}\n\\color{red}{+(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})}\n\\color{black}{+\\underline{S}} \\\\\n\n& \\color{blue}{=A^\\prime X^\\prime XA -A^\\prime X^\\prime X \\widehat{A} - \\widehat{A}^\\prime X^\\prime XA + \\widehat{A}^\\prime X^\\prime X \\widehat{A}}\n\\color{green}{ +A^\\prime \\underline{V}^{-1}A - A^\\prime \\underline{V}^{-1} \\underline{A} - \\underline{A}^\\prime \\underline{V}^{-1} A + \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}}\n\\color{red}{+ Y^\\prime Y-Y^\\prime X \\widehat{A} - \\widehat{A}^\\prime X^\\prime Y + \\widehat{A}^\\prime X^\\prime X \\widehat{A}}\n\\color{black}{+ \\underline{S}}\\\\\n& \\color{blue}{=A^\\prime X^\\prime XA -Y^\\prime XA - \\widehat{A}^\\prime X^\\prime XA}\n\\color{green}{ +A^\\prime \\underline{V}^{-1}A - A^\\prime \\underline{V}^{-1} \\underline{A} - \\underline{A}^\\prime \\underline{V}^{-1} A}\n\\color{red}{+ Y^\\prime Y}\n\\color{black}{+\\underline{S}}\n\\color{green}{+ \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}}\\\\\n& = A^\\prime (X^\\prime X+ \\underline{V}^{-1})A -2A^\\prime (X^\\prime Y+\\underline{V}^{-1} \\underline{A}) + Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}\n\\end{aligned}\\]\nWe can set \\(\\overline{V}^{-1}=X^\\prime X+ \\underline{V}^{-1}\\)\n\\[\n\\begin{aligned}\n& = A^\\prime \\overline{V}^{-1}A -2A^\\prime \\overline{V}^{-1}\\overline{V} (X^\\prime Y+\\underline{V}^{-1} \\underline{A}) + Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}\n\\end{aligned}\n\\]\nWe can set \\(\\overline{A}=\\overline{V}(X^\\prime Y+\\underline{V}^{-1}\\underline{A})\\)\n\\[\n\\begin{aligned}\n& = A^\\prime \\overline{V}^{-1}A -2A^\\prime \\overline{V}^{-1}\\overline{A} \\pm \\overline{A}^\\prime \\overline{V}^{-1}\\overline{A} + Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}\\\\\n& =(A-\\overline{A})^\\prime \\overline{V}^{-1}(A-\\overline{A})-\\overline{A}^\\prime \\overline{V}^{-1}\\overline{A}+Y^\\prime Y+\\underline{S}+\\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}\n\\end{aligned}\n\\]\nLet’s put the latter expression back in the \\(exp\\).\n\\[\n\\begin{aligned}\n\\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\overline{A})^{\\prime} \\overline{V}^{-1}(A-\\overline{A})\\right]\\right\\}\\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}\\overline{S}\\right]\\right\\}\n\\end{aligned}\n\\]\n\\[\n\\boxed{\n\\begin{array}{rcl}\n&p(A,\\Sigma \\mid Y,X)= p(A \\mid Y,X,\\Sigma)p(\\Sigma \\mid Y,X) \\\\\n&p(A \\mid Y,X,\\Sigma)=\\mathcal{M N}_{K\\times N}(\\overline{A},\\Sigma,\\overline{V})\\\\\n&p(\\Sigma \\mid Y,X)=\\mathcal{I W}_N (\\overline{S},\\overline{\\nu})\\\\\n& \\overline{V} = (X^\\prime X + \\underline{V}^{-1})^{-1}\\\\\n& \\overline{A}=\\overline{V}(X^\\prime Y+\\underline{V}^{-1}\\underline{A})\\\\\n& \\overline{\\nu}=T+\\underline{\\nu}\\\\\n& \\overline{S}=\\underline{S}+Y^\\prime Y+\\underline{A}^\\prime \\underline{V}^{-1}\\underline{A}-\\overline{A}^\\prime \\overline{V}^{-1}\\overline{A}\n\\end{array}\n}\n\\]\n\n\n\nThe Code for Bayesian VAR estimation\n\n# Bayesian estimation of the baseline model\n\n### Specify the setup\nN       = ncol(df.log)\np       = 4\nK       = 1+N*p\nS       = c(5000,50000)\nset.seed(123456)\n\n### Create Y and X matrices\ny       = ts(df.log, start=c(2004,1), frequency=12)\nY       = ts(y[5:nrow(y),], start=c(2004,2), frequency=12)\nX       = matrix(1,nrow(Y),1)\nfor (i in 1:p){\n  X     = cbind(X,y[5:nrow(y)-i,])\n}\n\nT       = nrow(Y)\n## MLE\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T\n\n## Specify the priors (Minnesota prior)\nkappa.1           = 0.02^2\nkappa.2           = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\n\npriors = list(\n  A.prior     = A.prior,\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior     = diag(diag(Sigma.hat)),\n  nu.prior    = N+1 \n)\n\n## BVAR function\n\nBVAR = function(Y,X,priors,S){\n  \n  # normal-inverse Wishard posterior parameters\n  V.bar.inv   = t(X)%*%X + diag(1/diag(priors$V.prior))\n  V.bar       = solve(V.bar.inv)\n  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(priors$V.prior))%*%priors$A.prior)\n  nu.bar      = nrow(Y) + priors$nu.prior\n  S.bar       = priors$S.prior + t(Y)%*%Y + t(priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n  S.bar.inv   = solve(S.bar)\n  \n  #posterior draws\n  Sigma.posterior   = rWishart(sum(S), df=nu.bar, Sigma=S.bar.inv)\n  Sigma.posterior   = apply(Sigma.posterior,3,solve)\n  Sigma.posterior   = array(Sigma.posterior,c(N,N,sum(S)))\n  A.posterior       = array(rnorm(prod(c(dim(A.bar),sum(S)))),c(dim(A.bar),sum(S)))\n  L                 = t(chol(V.bar))\n  \n  for (s in 1:sum(S)){\n    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n  }\n  \n  posterior = list(\n    Sigma.posterior   = Sigma.posterior,\n    A.posterior       = A.posterior\n  )\n  return(posterior)\n}\n\n\n## Apply function BVAR\nposterior.draws = BVAR(Y=Y, X=X, priors=priors, S=S)\n\nThe output of the BVAR function applied on the baseline model is:\n\nround(apply(posterior.draws$A.posterior, 1:2, mean),3)\n\n        [,1]  [,2]  [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]  [,11]\n [1,] -0.001 0.004 0.002 -0.006  0.032  0.136 0.004  0.046 -0.008  0.048  0.062\n [2,]  1.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n [3,]  0.000 1.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n [4,]  0.000 0.000 1.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n [5,]  0.000 0.000 0.000  1.000  0.000  0.000 0.000 -0.001 -0.001  0.000 -0.001\n [6,]  0.000 0.000 0.000  0.000  0.999  0.001 0.000 -0.001  0.001 -0.001 -0.003\n [7,]  0.000 0.000 0.000  0.001 -0.003  0.982 0.000  0.001  0.002  0.000  0.000\n [8,]  0.000 0.000 0.000  0.000  0.001  0.000 1.000  0.000  0.000  0.000  0.000\n [9,]  0.000 0.000 0.000  0.000  0.001  0.003 0.000  0.991 -0.010  0.008  0.000\n[10,]  0.000 0.000 0.000  0.000 -0.001  0.008 0.000 -0.004  0.971  0.009  0.004\n[11,]  0.000 0.000 0.000  0.000  0.000 -0.006 0.000  0.000  0.007  0.986 -0.009\n[12,]  0.000 0.000 0.000  0.001  0.001 -0.002 0.000 -0.015 -0.008  0.000  0.980\n[13,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[14,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[15,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[16,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[17,]  0.000 0.000 0.000  0.000 -0.001  0.001 0.000  0.000  0.001  0.000  0.000\n[18,]  0.000 0.000 0.000  0.000 -0.001 -0.005 0.000  0.001  0.002 -0.001  0.000\n[19,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[20,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000 -0.003 -0.001  0.000 -0.002\n[21,]  0.000 0.000 0.000  0.000  0.000  0.002 0.000 -0.001  0.001  0.000 -0.001\n[22,]  0.000 0.000 0.000  0.000  0.000 -0.001 0.000  0.002  0.002 -0.003  0.001\n[23,]  0.000 0.000 0.000  0.000  0.000 -0.001 0.000 -0.003  0.000 -0.001 -0.003\n[24,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[25,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[26,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[27,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[28,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[29,]  0.000 0.000 0.000  0.000  0.000 -0.002 0.000  0.001  0.001  0.000  0.000\n[30,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[31,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000 -0.001 -0.001\n[32,]  0.000 0.000 0.000  0.000 -0.001  0.001 0.000  0.000  0.000 -0.001  0.000\n[33,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.001  0.000  0.000  0.002\n[34,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.001 -0.001 -0.001  0.000\n[35,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[36,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[37,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[38,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[39,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[40,]  0.000 0.000 0.000  0.000  0.000 -0.001 0.000  0.001  0.001  0.000  0.001\n[41,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000  0.000\n[42,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.001  0.000 -0.001  0.000\n[43,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.001  0.001 -0.001  0.000\n[44,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000  0.001\n[45,]  0.000 0.000 0.000  0.000  0.000  0.000 0.000  0.001  0.000 -0.001  0.000\n\n\n\nround(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,]  0.000  0.000  0.000  0.000  0.000 -0.007  0.000  0.000  0.001  0.000\n [2,]  0.000  0.001  0.000  0.000  0.000  0.007  0.000 -0.001  0.000  0.001\n [3,]  0.000  0.000  0.000  0.000  0.000 -0.001  0.000  0.000  0.001 -0.001\n [4,]  0.000  0.000  0.000  0.003  0.000 -0.012  0.000 -0.004  0.001  0.002\n [5,]  0.000  0.000  0.000  0.000  0.045  0.000  0.000 -0.007  0.005  0.012\n [6,] -0.007  0.007 -0.001 -0.012  0.000  0.548 -0.001 -0.028 -0.032  0.024\n [7,]  0.000  0.000  0.000  0.000  0.000 -0.001  0.000  0.000  0.000  0.000\n [8,]  0.000 -0.001  0.000 -0.004 -0.007 -0.028  0.000  0.523  0.444 -0.084\n [9,]  0.001  0.000  0.001  0.001  0.005 -0.032  0.000  0.444  1.201 -0.308\n[10,]  0.000  0.001 -0.001  0.002  0.012  0.024  0.000 -0.084 -0.308  0.332\n[11,] -0.001  0.000  0.000 -0.006 -0.004  0.023  0.000  0.473  0.256  0.134\n       [,11]\n [1,] -0.001\n [2,]  0.000\n [3,]  0.000\n [4,] -0.006\n [5,] -0.004\n [6,]  0.023\n [7,]  0.000\n [8,]  0.473\n [9,]  0.256\n[10,]  0.134\n[11,]  0.751"
  },
  {
    "objectID": "index.html#the-extended-model",
    "href": "index.html#the-extended-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "The extended model",
    "text": "The extended model\n\nThe prior distribution\n\\[\n\\begin{array}{rcl}\n&A \\mid \\Sigma,{\\color{red}\\kappa} \\sim  \\mathcal{M N} _{T \\times N}(\\underline{A}, \\Sigma, {\\color{red}\\kappa}\\underline{V}) \\\\\n&\\color{red}{\\kappa \\sim \\mathcal{IG2}(\\underline{s}_\\kappa,\\underline{\\nu}_\\kappa)}\\\\\n&\\Sigma \\sim \\mathcal{IW}_{N}(\\underline{S}, \\underline{\\nu})\n\\end{array}\n\\]\n\n\nThe posterior distribution\nIn this section, we will derive the the joint full-conditional posterior distribution of \\(A\\) and \\(\\Sigma\\) and the full-conditional posterior distribution of \\(\\kappa\\).\n\nThe Derivations of the joint full-conditional posterior distribution of A and Sigma\n\\[\n\\begin{aligned}\np(A,\\Sigma \\mid Y,X,{\\color{red}\\kappa}) \\propto L(A, \\Sigma \\mid Y,X) p(A,\\Sigma) = L(A, \\Sigma \\mid Y,X) p(A\\mid\\Sigma,{\\color{red}\\kappa})p(\\Sigma)p({\\color{red}\\kappa})\n\\end{aligned} 4\n\\] Let’s focus on the kernel\n\\[\\begin{aligned}\np(A,\\Sigma \\mid Y,X,{\\color{red}{\\kappa}}) \\propto & det (\\Sigma)^{-\\frac{T}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})\\right]\\right\\} \\\\\n& \\times det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1} \\underline{S}\\right]\\right\\}\\\\\n\n& = \\operatorname{det}(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\{-\\frac{1}{2} \\operatorname{tr}[\\Sigma^{-1} [(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})+(A-\\underline{A})^{\\prime} \\frac{1}{\\kappa} \\underline{V}^{-1}(A-\\underline{A}) \\\\\n& +(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})+\\underline{S}]]\\}\n\n\\end{aligned}\\]\n\\[\\begin{aligned}\n&\n(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})\n+(A-\\underline{A})^{\\prime} {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1}(A-\\underline{A})\n+(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})\n+\\underline{S} \\\\\n\n& =A^\\prime X^\\prime XA -A^\\prime X^\\prime X \\widehat{A} - \\widehat{A}^\\prime X^\\prime XA + \\widehat{A}^\\prime X^\\prime X \\widehat{A}\n+A^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1}A - A^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A} - \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} A + \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A}\n+ Y^\\prime Y-Y^\\prime X \\widehat{A} - \\widehat{A}^\\prime X^\\prime Y + \\widehat{A}^\\prime X^\\prime X \\widehat{A}+ \\underline{S}\\\\\n\n& =A^\\prime X^\\prime XA -Y^\\prime XA - \\widehat{A}^\\prime X^\\prime XA\n+A^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1}A - A^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A} - \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} A\n+ Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A}\\\\\n& = A^\\prime (X^\\prime X+ {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1})A -2A^\\prime (X^\\prime Y+{\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A}) + Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\boxed{\n\\begin{array}{rcl}\n&p(A,\\Sigma \\mid Y,X,{\\color{red}\\kappa})= \\mathcal{MNIW}_{K\\times N}(\\overline{A},\\overline{V},\\overline{S},\\overline{\\nu}) \\\\\n& \\overline{V} = (X^\\prime X + {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1})^{-1}\\\\\n& \\overline{A}=\\overline{V}(X^\\prime Y+{\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1}\\underline{A})\\\\\n& \\overline{\\nu}=T+\\underline{\\nu}\\\\\n& \\overline{S}=\\underline{S}+Y^\\prime Y+\\underline{A}^\\prime \\underline{V}^{-1}{\\color{red}{\\frac{1}{\\kappa}}}\\underline{A}-\\overline{A}^\\prime \\overline{V}^{-1}\\overline{A}\n\\end{array}\n}\n\\end{aligned}\\]\n\n\nThe Derivations of the full-conditional posterior distribution of \\(\\kappa\\)\n\\[\\begin{align}\np(\\kappa \\mid A, \\Sigma, Y,X) & =L(A,\\Sigma \\mid Y,X)p(\\kappa \\mid \\underline{s}_{\\kappa}, \\underline{\\nu}_{\\kappa})p(A,\\Sigma)\\\\\n& =L(A,\\Sigma \\mid Y,X)p(\\kappa \\mid \\underline{s}_{\\kappa}, \\underline{\\nu}_{\\kappa})p(A\\mid \\Sigma)p(\\Sigma)\\\\\n\n&= (\\kappa)^{-\\frac{\\underline{\\nu}+2}{2}}\\exp\\left\\{\\frac{-1}{2}\\frac{\\underline{s}_{\\kappa}}{\\kappa}\\right\\} \\\\\n& \\times det(\\kappa \\underline{V})^{-\\frac{N}{2}}\n\\exp\\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\frac{1}{\\kappa}\\underline{V}^{-1}(A-\\underline{A})\\right]\\right\\} \\\\\n\n&= (\\kappa)^{-\\frac{\\underline{\\nu}+2}{2}}(\\kappa)^{-\\frac{KN}{2}}\\kappa ^{-\\frac{\\underline{\\nu}+KN+2}{2}}\\times \\exp\\left\\{\\frac{-1}{2} \\frac{1}{\\kappa}(\\underline{s}_{\\kappa}+\\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right])\\overline{s}_{\\kappa}\\right\\}\\\\\n\n&= \\kappa ^{-\\frac{\\overbrace{\\underline{\\nu}+KN}^{\\overline{\\nu}_{\\kappa}}+2}{2}}\\times \\exp\\left\\{\\frac{-1}{2} \\frac{1}{\\kappa}\\underbrace{(\\underline{s}_{\\kappa}+\\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right])}_{\\overline{s}_{\\kappa}}\\right\\}\n\n\\end{align}\\]\nWe obtain the following full-conditional posterior distribution of \\(\\kappa\\):\n\\[\n\\boxed{\n\\begin{array}{rcl}\n&\\kappa \\sim \\mathcal{IG2}(\\overline{s}_{\\kappa},\\overline{\\nu}_{\\kappa})\\\\\n& \\overline{s}_{\\kappa}=\\underline{s}_{\\kappa}+\\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right]\\\\\n&\\overline{\\nu}_{\\kappa}=\\underline{\\nu}+KN\n\\end{array}\n}\n\\]\n\n\n\nThe Code for Bayesian VAR estimation\nLet’s adapt the BVAR function to the extended model.\n\n####Extended model\n\n### Modify the priors\nS       = c(5000,50000)\nkappa.1           = 1\nkappa.2           = 10\ninitial_kappa     = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\n\npriors = list(\n  A.prior            = A.prior,\n  V.prior            = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior            = diag(diag(Sigma.hat)),\n  nu.prior           = N+1,\n  s.kappa.prior      = 2,\n  nu.kappa.prior     = 4\n)\n\n### Modify BVAR function\n\nBVAR_extension = function(X,Y,priors,initial_kappa,S){\n  \n  A.posterior        = array(NA, dim = c(K,N,sum(S)))\n  Sigma.posterior    = array(NA,dim=c(N,N,sum(S)))\n  kappa.posterior    = matrix(NA, sum(S), 1) \n  kappa.posterior[1] = initial_kappa\n  \n  for (s in 1:sum(S)){\n    # full-cond of joint posterior of A and Sigma\n    V.bar.inv   = t(X)%*%X + diag(1/diag(kappa.posterior[s]*priors$V.prior))\n    V.bar       = solve(V.bar.inv)\n    A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(kappa.posterior[s]*priors$V.prior))%*%priors$A.prior)\n    nu.bar      = nrow(Y) + priors$nu.prior\n    S.bar       = priors$S.prior + t(Y)%*%Y + t(priors$A.prior)%*%diag(1/diag(kappa.posterior[s]*priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv   = solve(S.bar)\n    \n    \n    Sigma.posterior.dist   = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.draw             = apply(Sigma.posterior.dist,3,solve)\n    Sigma.posterior[,,s]   = Sigma.draw\n    A.posterior[,,s]            = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))\n    L                      = t(chol(V.bar))\n    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n    \n    #full conditional posterior of kappa\n    if (s!=sum(S)){\n      s.kappa.bar           = priors$s.kappa.prior + sum(diag(solve( Sigma.posterior[,,s])*t(A.posterior[,,s]-priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%(A.posterior[,,s]-priors$A.prior)))\n      nu.kappa.bar          = priors$nu.kappa.prior + (K*N)\n      kappa.draw            = s.kappa.bar/rchisq(1, df=nu.kappa.bar)\n      kappa.posterior[s+1]  = kappa.draw\n    }\n  }\n  \n  posterior.extension = list(\n    Sigma.posterior   = Sigma.posterior[,,S[1]+1:S[2]], #getting rid of first S[1] draws\n    A.posterior       = A.posterior[,,S[1]+1:S[2]],\n    kappa.posterior   = kappa.posterior[S[1]+1:S[2],1]\n  )\n  return(posterior.extension)\n}\n\n## Apply function BVAR_extension\nposterior.ext.draws = BVAR_extension(Y=Y, X=X, priors=priors, initial_kappa=initial_kappa, S=S)\n\nThe output of the BVAR function applied on the baseline model is:\n\nround(apply(posterior.ext.draws$Sigma.posterior, 1:2, mean),3)\n\n        [,1]   [,2] [,3]   [,4]   [,5]   [,6] [,7]   [,8]   [,9]  [,10]  [,11]\n [1,]  0.000  0.000    0  0.000  0.000 -0.004    0  0.000  0.000  0.000  0.000\n [2,]  0.000  0.000    0  0.000  0.000  0.002    0 -0.001 -0.001  0.000  0.000\n [3,]  0.000  0.000    0  0.000  0.000  0.000    0  0.000  0.000  0.000  0.000\n [4,]  0.000  0.000    0  0.002  0.001 -0.006    0  0.001  0.001  0.001  0.001\n [5,]  0.000  0.000    0  0.001  0.030  0.001    0 -0.004  0.010 -0.001 -0.005\n [6,] -0.004  0.002    0 -0.006  0.001  0.283    0 -0.021  0.021 -0.018 -0.017\n [7,]  0.000  0.000    0  0.000  0.000  0.000    0  0.000  0.000  0.000  0.000\n [8,]  0.000 -0.001    0  0.001 -0.004 -0.021    0  0.337  0.347 -0.026  0.307\n [9,]  0.000 -0.001    0  0.001  0.010  0.021    0  0.347  0.746 -0.119  0.272\n[10,]  0.000  0.000    0  0.001 -0.001 -0.018    0 -0.026 -0.119  0.115  0.018\n[11,]  0.000  0.000    0  0.001 -0.005 -0.017    0  0.307  0.272  0.018  0.392\n\nround(apply(posterior.ext.draws$A.posterior, 1:2, mean),3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]    [,6]   [,7]    [,8]    [,9]   [,10]\n [1,]  0.946  1.551  0.039  3.598 15.017 -15.672  0.034 -15.442 107.938 -27.315\n [2,]  1.613 -1.327  0.101  1.351  1.876 -67.556  0.064   4.435  -7.041   2.583\n [3,] -0.102  0.388 -0.005 -0.218 -0.823   7.196 -0.024  -0.677   4.947  -3.017\n [4,]  0.129 -0.011  1.235 -0.788  7.931  -8.448  0.043   7.413  11.516 -13.749\n [5,]  0.020  0.031  0.002  0.896  0.524  -0.857  0.013   0.946   0.282  -0.566\n [6,]  0.002 -0.006  0.000 -0.006  0.917  -0.170 -0.002  -0.338  -0.539   0.096\n [7,]  0.016 -0.017  0.002  0.016  0.045  -0.132  0.001  -0.023  -0.271   0.108\n [8,]  0.077 -0.363  0.038  0.802 -4.423  -5.451  1.348   0.349   3.844  -6.482\n [9,]  0.002 -0.008 -0.001 -0.012 -0.006  -0.100  0.000   0.922   0.720   0.243\n[10,] -0.002 -0.002  0.000 -0.001 -0.016   0.082  0.000   0.025   0.467  -0.058\n[11,]  0.000 -0.006  0.000  0.010 -0.069  -0.122  0.000   0.063   0.162   0.523\n[12,]  0.001  0.007  0.000  0.020  0.041  -0.050  0.001  -0.192  -0.319  -0.169\n[13,] -0.598  0.495 -0.034 -1.087 -0.844  48.839 -0.029  -5.283 -14.891   1.176\n[14,]  0.065  0.470  0.008  0.391 -0.699  -0.209 -0.001  -2.776  -1.139  -0.051\n[15,] -0.034  0.140 -0.190  0.530 -3.807   3.469 -0.018   2.740   0.717   2.650\n[16,] -0.035  0.012 -0.007 -0.236 -0.025   1.585  0.003  -0.139  -0.542   1.218\n[17,]  0.002 -0.005  0.000  0.015 -0.171  -0.193  0.000   0.252   0.471  -0.100\n[18,] -0.013  0.009 -0.001 -0.019 -0.008   0.943 -0.001   0.036  -0.064  -0.016\n[19,] -0.039  0.159 -0.007 -0.667  0.420   2.618 -0.189  -2.702  -9.125   4.096\n[20,] -0.004  0.002  0.000  0.017 -0.020   0.240 -0.001  -0.329  -0.799   0.318\n[21,]  0.001  0.001  0.001 -0.012  0.031  -0.033  0.000   0.178   0.487  -0.178\n[22,]  0.002  0.002  0.002 -0.019  0.001   0.005 -0.001   0.144   0.328  -0.438\n[23,]  0.003  0.001 -0.001  0.002  0.005  -0.162  0.000  -0.024   0.317  -0.039\n[24,] -0.105  0.247 -0.019  0.311  1.001  10.886 -0.029  -0.620   2.298  -2.118\n[25,]  0.047  0.042 -0.001  0.231 -0.713  -0.412  0.018  -0.387   0.736   4.845\n[26,] -0.009  0.060 -0.070 -0.368 -2.163   1.703 -0.003   2.621  -4.593   3.641\n[27,]  0.037  0.013 -0.005  0.138 -0.192  -1.382 -0.002  -0.730  -2.409  -0.227\n[28,] -0.004  0.008 -0.001  0.013  0.102   0.512  0.000  -0.189  -0.149   0.025\n[29,] -0.003  0.010 -0.001 -0.006 -0.006   0.217 -0.001  -0.132  -0.033  -0.067\n[30,]  0.010  0.159 -0.007 -0.275  2.779   0.977 -0.110  -3.167  -0.914   2.327\n[31,]  0.001 -0.001  0.001 -0.006 -0.005  -0.043  0.000  -0.113   0.805  -0.269\n[32,] -0.002  0.002  0.000  0.000 -0.035   0.076  0.000   0.116  -0.107  -0.001\n[33,] -0.001  0.006  0.000  0.014 -0.053  -0.122  0.000   0.224   0.394  -0.047\n[34,]  0.002  0.000 -0.001  0.001  0.036  -0.031  0.000  -0.022  -0.713   0.348\n[35,] -0.035  0.346  0.006 -0.676 -1.379   6.096 -0.024  -0.475   2.635   0.966\n[36,] -0.033 -0.008 -0.035 -0.235 -1.256  -1.796 -0.007  -0.477  -2.999   0.547\n[37,] -0.050  0.062 -0.035 -0.151 -0.777   2.292  0.020   4.259  -1.475   3.311\n[38,] -0.007  0.000  0.004  0.033 -0.068  -0.154 -0.005   1.026   2.265  -0.568\n[39,]  0.002 -0.001  0.001 -0.009 -0.085  -0.260 -0.001   0.139   0.142  -0.127\n[40,]  0.002 -0.002  0.000  0.003 -0.021  -0.176  0.000   0.125   0.279   0.001\n[41,] -0.012  0.109 -0.012  0.144  2.474   1.153 -0.040   4.033   8.282   0.263\n[42,] -0.001 -0.009 -0.002 -0.004  0.001   0.084  0.000  -0.036  -0.465   0.344\n[43,]  0.002  0.001  0.000 -0.001 -0.015  -0.097  0.000   0.083   0.076   0.029\n[44,]  0.001  0.001 -0.002 -0.001  0.025   0.103  0.000   0.011  -0.736   0.469\n[45,] -0.001  0.005  0.002  0.004 -0.002   0.051  0.000   0.107   0.477  -0.480\n        [,11]\n [1,] -38.451\n [2,]   2.883\n [3,]  -2.903\n [4,]  -5.569\n [5,]  -0.023\n [6,]  -0.242\n [7,]   0.030\n [8,]  -1.159\n [9,]   0.603\n[10,]   0.024\n[11,]   0.006\n[12,]   0.045\n[13,]  -1.662\n[14,]  -2.423\n[15,]  10.543\n[16,]   1.665\n[17,]   0.023\n[18,]   0.024\n[19,]  -4.030\n[20,]  -0.186\n[21,]   0.036\n[22,]  -0.219\n[23,]   0.070\n[24,]  -2.776\n[25,]   1.405\n[26,]   8.582\n[27,]  -1.789\n[28,]  -0.138\n[29,]  -0.164\n[30,]  -3.237\n[31,]  -0.240\n[32,]   0.171\n[33,]   0.216\n[34,]   0.182\n[35,]  -0.264\n[36,]  -0.379\n[37,]   8.251\n[38,]   1.950\n[39,]   0.098\n[40,]   0.153\n[41,]   6.388\n[42,]  -0.087\n[43,]   0.195\n[44,]   0.307\n[45,]  -0.155\n\nround(mean(posterior.ext.draws$kappa.posterior),3)\n\n[1] 2137.854"
  },
  {
    "objectID": "index.html#the-likelihood-function-1",
    "href": "index.html#the-likelihood-function-1",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "The Likelihood function",
    "text": "The Likelihood function\n\\[\\begin{aligned}\nY \\mid X,A,\\Sigma & \\sim \\mathcal{M N} _{T \\times N}\\left(XA, \\Sigma, diag(\\sigma^2)\\right)\n\\end{aligned}\\]\n\\[\\begin{aligned}\nL(A, \\Sigma \\mid Y, X) & \\propto det(\\Sigma)^{-\\frac{T}{2}} \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X A)^{\\prime}diag(\\sigma^2)^{-1}(Y-X A)\\right]\\right\\} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "index.html#the-posterior-distribution-2",
    "href": "index.html#the-posterior-distribution-2",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "The posterior distribution",
    "text": "The posterior distribution\nIf we use the same prior distributions as in the baseline model and the above likelihood function, we obtain the following full conditional joint posterior distribution:"
  },
  {
    "objectID": "index.html#the-code-for-bayesian-var-estimation-2",
    "href": "index.html#the-code-for-bayesian-var-estimation-2",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "The Code for Bayesian VAR estimation",
    "text": "The Code for Bayesian VAR estimation\n\nlibrary(mgcv)\n\nSVcommon.Gibbs.iteration = function(aux, priors){\n  # A single iteration of the Gibbs sampler for the SV component\n  #\n  # aux is a list containing:\n  #   Y - a TxN matrix\n  #   X - a TxK matrix\n  #   H - a Tx1 matrix\n  #   h0 - a scalar\n  #   sigma.v2 - a scalar\n  #   s - a Tx1 matrix\n  #   A - a KxN matrix\n  #   Sigma - an NxN matrix\n  #   sigma2 - a Tx1 matrix\n  #\n  # priors is a list containing:\n  #   h0.v - a positive scalar\n  #   h0.m - a scalar\n  #   sigmav.s - a positive scalar\n  #   sigmav.nu - a positive scalar\n  #   HH - a TxT matrix\n  \n  T             = dim(aux$Y)[1]\n  N             = dim(aux$Y)[2]\n  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)\n  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)\n  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)\n  \n  Lambda        = solve(chol(aux$Sigma))\n  Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)\n  Y.tilde       = as.vector(log((Z + 0.0000001)^2))\n  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])\n  \n  # sampling initial condition\n  ############################################################\n  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))\n  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))\n  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))\n  aux$h0        = h0.draw\n  \n  # sampling sigma.v2\n  ############################################################\n  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)\n  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)\n  aux$sigma.v2  = sigma.v2.draw\n  \n  # sampling auxiliary states\n  ############################################################\n  Pr.tmp        = simplify2array(lapply(1:10,function(x){\n    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])\n  }))\n  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))\n  s.cum         = t(apply(Pr, 1, cumsum))\n  r             = matrix(rep(runif(T), 10), ncol = 10)\n  ss            = apply(s.cum &lt; r, 1, sum) + 1\n  aux$s         = as.matrix(ss)\n  \n  \n  # sampling log-volatilities using functions for tridiagonal precision matrix\n  ############################################################\n  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])\n  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH\n  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])\n  lead.diag     = diag(D.inv)\n  sub.diag      = mgcv::sdiag(D.inv, -1)\n  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)\n  D.L           = diag(D.chol$ld)\n  mgcv::sdiag(D.L,-1) = D.chol$sd\n  x             = as.matrix(rnorm(T))\n  a             = forwardsolve(D.L, b)\n  draw          = backsolve(t(D.L), a + x)\n  aux$H         = as.matrix(draw)\n  aux$sigma2    = as.matrix(exp(draw))\n  \n  return(aux)\n}\n\n\n### Specify the setup\nlibrary(progress)\nN       = ncol(df.log)\np       = 4\nK       = 1+N*p\nS       = c(3000,30000)\nh       = 20\nset.seed(123456)\n\n### Create Y and X matrices\ny       = ts(df.log, start=c(2004,1), frequency=12)\nY       = ts(y[5:nrow(y),], start=c(2004,2), frequency=12)\nX       = matrix(1,nrow(Y),1)\nfor (i in 1:p){\n  X     = cbind(X,y[5:nrow(y)-i,])\n}\n\nT = nrow(Y)\n### MLE\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n### Specify the priors (Minnesota prior)\nkappa.1           = 0.02^2\nkappa.2           = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\nH                 = diag(T)\nmgcv::sdiag(H,-1) = -1\nHH                = 2*diag(T)\nmgcv::sdiag(HH,-1)      = -1\nmgcv::sdiag(HH,1)       = -1\n\n\npriors = list(\n  A.prior     = A.prior,\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior     = diag(diag(Sigma.hat)),\n  nu.prior    = N+1, \n  #add new priors\n  h0.v = 1, #   h0.v - a positive scalar\n  h0.m = 0, #   h0.m - a scalar\n  sigmav.s = 1,#   sigmav.s - a positive scalar\n  sigmav.nu =1, #   sigmav.nu - a positive scalar\n  HH = HH #   HH - a TxT matrix\n  )\n\n\n### SV_BVAR function\n\nSV_bvar = function(Y,X,priors,S){\n  \n  A.posterior        = array(NA, dim = c(K,N,sum(S)))\n  Sigma.posterior    = array(NA,dim=c(N,N,sum(S)))\n  sigma2.posterior    = matrix(NA, nrow(Y), sum(S)) \n  \n  aux &lt;- list(\n    Y = Y, #   Y - a TxN matrix\n    X = X,  #   X - a TxK matrix\n    H = matrix(1,T,1),   #H - a Tx1 matrix\n    h0 = 0, #h0 - a scalar\n    sigma.v2 = 1, #   sigma.v2 - a scalar\n    s = matrix(1,T,1), #   s - a Tx1 matrix\n    A = matrix(0, K, N), #   A - a KxN matrix\n    Sigma = diag(diag(matrix(1, N, N))), #   Sigma - an NxN matrix\n    sigma2 = matrix(1, T, 1) #   sigma2 - a Tx1 matrix\n    )\n  \n  #Progress bar\n  pb &lt;- progress_bar$new(format = \"(:spin) [:bar] :percent [Elapsed time: :elapsedfull || Estimated time remaining: :eta]\",\n                         total = sum(S),\n                         complete = \"=\",   # Completion bar character\n                         incomplete = \"-\", # Incomplete bar character\n                         current = \"&gt;\",    # Current bar character\n                         clear = FALSE,    # If TRUE, clears the bar when finish\n                         width = 100)      # Width of the progress bar\n  \n  for (s in 1:sum(S)){\n    pb$tick() #for progress bar\n    \n    # full-cond of joint posterior of A and Sigma\n    V.bar.inv   = t(X)%*%diag(1/as.vector(aux$sigma2))%*%X + diag(1/diag(priors$V.prior))\n    V.bar       = solve(V.bar.inv)\n    A.bar       = V.bar%*%(t(X)%*%diag(1/as.vector(aux$sigma2))%*%Y + diag(1/diag(priors$V.prior))%*%priors$A.prior)\n    nu.bar      = nrow(Y) + priors$nu.prior\n    S.bar       = priors$S.prior + t(Y)%*%diag(1/as.vector(aux$sigma2))%*%Y + t(priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv   = solve(S.bar)\n    \n    \n    Sigma.posterior.dist   = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.draw             = apply(Sigma.posterior.dist,3,solve)\n    Sigma.posterior[,,s]   = Sigma.draw\n    A.posterior[,,s]            = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))\n    L                      = t(chol(V.bar))\n    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n    \n    #draw from SV common gibbs sampler\n    aux = SVcommon.Gibbs.iteration(aux, priors)\n    sigma2.posterior[,s]  = aux$sigma2\n    \n    \n  }\n  \n  posterior.sv = list(\n    Sigma.posterior   = Sigma.posterior[,,S[1]+1:S[2]], #getting rid of first S[1] draws\n    A.posterior       = A.posterior[,,S[1]+1:S[2]],\n    Sigma2.posterior   = sigma2.posterior[,S[1]+1:S[2]]\n  )\n  return(posterior.sv)\n}\n\n\n## Apply function BVAR\nposterior.sv.draws = SV_bvar(Y=Y, X=X, priors=priors, S=S)\n\nThe output of the function is:\n\nround(apply(posterior.sv.draws$Sigma.posterior, 1:2, mean),3)\n\n      [,1] [,2] [,3] [,4] [,5]  [,6] [,7]  [,8]   [,9]  [,10] [,11]\n [1,]    0    0    0    0    0 0.000    0 0.000  0.000  0.000 0.000\n [2,]    0    0    0    0    0 0.000    0 0.000  0.000  0.000 0.000\n [3,]    0    0    0    0    0 0.000    0 0.000  0.000  0.000 0.000\n [4,]    0    0    0    0    0 0.000    0 0.000  0.000  0.000 0.000\n [5,]    0    0    0    0    0 0.000    0 0.000  0.000  0.000 0.000\n [6,]    0    0    0    0    0 0.002    0 0.000  0.000  0.000 0.000\n [7,]    0    0    0    0    0 0.000    0 0.000  0.000  0.000 0.000\n [8,]    0    0    0    0    0 0.000    0 0.004  0.002  0.000 0.003\n [9,]    0    0    0    0    0 0.000    0 0.002  0.008 -0.001 0.001\n[10,]    0    0    0    0    0 0.000    0 0.000 -0.001  0.002 0.001\n[11,]    0    0    0    0    0 0.000    0 0.003  0.001  0.001 0.006\n\nround(apply(posterior.sv.draws$A.posterior, 1:2, mean),3)\n\n       [,1]  [,2]  [,3]  [,4] [,5]   [,6]  [,7]  [,8]  [,9] [,10] [,11]\n [1,] 0.002 0.002 0.003 0.001 0.01 -0.003 0.005 0.081 0.107 0.032   0.1\n [2,] 1.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n [3,] 0.000 1.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n [4,] 0.000 0.000 1.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n [5,] 0.000 0.000 0.000 1.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n [6,] 0.000 0.000 0.000 0.000 1.00  0.000 0.000 0.000 0.000 0.000   0.0\n [7,] 0.000 0.000 0.000 0.000 0.00  1.000 0.000 0.000 0.000 0.000   0.0\n [8,] 0.000 0.000 0.000 0.000 0.00  0.000 1.000 0.000 0.000 0.000   0.0\n [9,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 1.000 0.000 0.000   0.0\n[10,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 1.000 0.000   0.0\n[11,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 1.000   0.0\n[12,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   1.0\n[13,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[14,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[15,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[16,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[17,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[18,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[19,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[20,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[21,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[22,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[23,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[24,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[25,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[26,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[27,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[28,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[29,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[30,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[31,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[32,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[33,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[34,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[35,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[36,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[37,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[38,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[39,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[40,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[41,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[42,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[43,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[44,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n[45,] 0.000 0.000 0.000 0.000 0.00  0.000 0.000 0.000 0.000 0.000   0.0\n\nround(apply(posterior.sv.draws$Sigma2.posterior, 1, mean),3)\n\n  [1]   23.610   35.230   47.138   59.928   74.086   86.606   97.517  106.591\n  [9]  114.581  122.339  131.623  139.376  146.878  152.258  154.562  158.262\n [17]  168.819  178.924  188.377  195.108  203.340  213.013  225.860  238.984\n [25]  249.758  253.718  260.528  271.151  286.358  297.145  310.405  322.972\n [33]  334.125  350.598  367.022  381.854  402.951  420.748  436.532  454.661\n [41]  472.659  493.153  515.358  535.799  550.315  572.133  597.294  615.004\n [49]  638.838  666.443  689.716  715.803  747.758  778.272  813.818  845.611\n [57]  877.066  917.493  954.680 1001.435 1036.034 1080.799 1108.769 1141.213\n [65] 1172.886 1206.389 1232.790 1242.037 1251.000 1247.159 1249.122 1251.107\n [73] 1242.846 1236.034 1227.207 1213.242 1208.300 1191.507 1163.102 1124.955\n [81] 1076.473 1035.664 1004.174  988.135  970.404  953.611  932.945  923.232\n [89]  910.478  904.723  891.226  877.730  872.643  873.049  879.021  874.571\n [97]  880.885  885.323  883.716  887.942  892.129  895.899  904.835  920.591\n[105]  926.893  933.565  940.357  947.348  955.025  972.004  984.185 1012.742\n[113] 1024.824 1040.738 1049.283 1054.008 1064.440 1080.742 1104.057 1120.021\n[121] 1135.693 1150.845 1161.163 1171.797 1187.509 1202.167 1223.110 1225.133\n[129] 1222.795 1228.555 1232.558 1248.127 1259.267 1251.771 1241.010 1235.189\n[137] 1218.746 1200.466 1166.826 1115.387 1048.892  997.984  961.018  922.853\n[145]  896.523  875.088  855.807  835.144  827.859  822.321  819.204  806.767\n[153]  798.830  794.161  787.454  789.356  786.188  784.502  786.903  784.976\n[161]  778.865  767.869  773.497  786.328  792.924  804.625  809.510  812.180\n[169]  821.890  832.329  849.919  849.416  858.666  850.336  832.047  829.338\n[177]  827.640  817.942  812.116  809.502  804.906  801.741  790.144  785.703\n[185]  787.898  799.867  807.540  830.303  836.397  848.682  874.287  900.224\n[193]  912.155  915.493  931.189  938.130  946.817  951.512  955.971  955.880\n[201]  951.831  946.988  935.963  923.677  912.216  901.170  894.854  888.321\n[209]  877.532  866.782  866.079  864.413  863.562  858.528  846.608  817.800\n[217]  785.911  728.262  665.213  586.061  497.302  393.872  281.156  170.356\n[225]   79.736   22.827"
  },
  {
    "objectID": "index.html#forecasting-with-the-baseline-model",
    "href": "index.html#forecasting-with-the-baseline-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Forecasting with the baseline model",
    "text": "Forecasting with the baseline model\nTo build the point forecasts, we use numerical integration to sample from the joint predictive density in the following steps:\n\nWe sample posterior draws from \\(p(A,\\Sigma \\mid Y,X)\\) by using the BVAR estimation sampler created above.\nWe obtain \\(\\left\\{ A^{(s)},\\Sigma^{(s)} \\right\\}^S_{s=1}\\)\nWe sample draws from \\(\\hat{p}(Y_{t+h}\\mid Y_t)\\) by:\n\n\\[\\begin{align}\nY^{(s)}_{t+h} \\sim \\mathcal{N}_{hN}(Y_{t+h\\mid t}(A^{(s)}), Var[Y_{t+h \\mid t} \\mid A^{(s)}, \\Sigma ^{(s)}]))\n\\end{align}\\]\n\nWe obtain \\(\\left\\{Y^{(s)}_{t+h}\\right\\}^{S}_{s=1}\\)\nCharacterise of the predictive density using \\(\\left\\{Y^{(s)}_{t+h}\\right\\}^{S}_{s=1}\\)\n\n\n# simulate draws from the predictive density\nlibrary(mvtnorm)\nlibrary(progress)\nh = 36\nS = 45000\nY.h         = array(NA,c(h,N,S))\n\n#Progress bar\npb &lt;- progress_bar$new(format = \"(:spin) [:bar] :percent [Elapsed time: :elapsedfull || Estimated time remaining: :eta]\",\n                       total = S,\n                       complete = \"=\",   # Completion bar character\n                       incomplete = \"-\", # Incomplete bar character\n                       current = \"&gt;\",    # Current bar character\n                       clear = FALSE,    # If TRUE, clears the bar when finish\n                       width = 100)      # Width of the progress bar\n\nfor (s in 1:S){\n  pb$tick() #for progress bar\n  x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]\n  x.Ti        = x.Ti[p:1,]\n  for (i in 1:h){\n    x.T         = c(1,as.vector(t(x.Ti)))\n    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%posterior.draws$A.posterior[,,s], sigma=posterior.draws$Sigma.posterior[,,s])\n    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])\n  }\n}\n# plots of forecasts\nlibrary(plot3D)\nlibrary(MASS)\nlibrary(HDInterval)\n\npce.point.f    = apply(Y.h[,1,],1,mean) #one pce forecasts\npce.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)\npce.range      = range(y[,1],pce.interval.f)\n\nblue  = \"#05386B\"\nplum      =\"#BEBADA\"\nplum.rgb = col2rgb(\"thistle\")\nshade = rgb(plum.rgb[1],plum.rgb[2],plum.rgb[3],maxColorValue=255,alpha=100, names=\"thistle\")\n\n\npar(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)\nplot(1:(length(y[,1])+h),c(y[,1],pce.point.f), type=\"l\", ylim=pce.range, axes=FALSE, xlab=\"\", ylab=\"\", lwd=2, col=plum)\naxis(1,c(1,49,97,145,193, nrow(y),nrow(y)+h),c(\"2004-01\",\"2008-01\",\"2012-01\",\"2016-01\",\"2020-01\",\"\",\"\"), col=blue)\naxis(2,c(pce.range[1],mean(pce.range),pce.range[2]),c(\"\",\"PCE\",\"\"), col=blue)\nabline(v=nrow(y), col=\"gray42\")\ntext(x=228, y=9.65, srt=90, \"2023-02\")\nabline(v=nrow(y)+12, col=\"gray42\")\ntext(x=238, y=9.65, srt=90, \"2024-02\")\nabline(v=nrow(y)+24, col=\"gray42\")\ntext(x=250, y=9.65, srt=90, \"2025-02\")\npolygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[37:1]),\n        c(y[230,1],pce.interval.f[1,],pce.interval.f[2,36:1],y[230,1]),\n        col=shade, border=plum)\n\n\n\ndev.off()\n\nnull device \n          1"
  },
  {
    "objectID": "index.html#forecasting-with-the-extended-model",
    "href": "index.html#forecasting-with-the-extended-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Forecasting with the extended model",
    "text": "Forecasting with the extended model\nWe can repeat the same process for the extended model.\n\n# simulate draws from the predictive density\nlibrary(mvtnorm)\nlibrary(progress)\nh = 36\nS = 45000\nY.h         = array(NA,c(h,N,S))\n\n#Progress bar\npb &lt;- progress_bar$new(format = \"(:spin) [:bar] :percent [Elapsed time: :elapsedfull || Estimated time remaining: :eta]\",\n                       total = S,\n                       complete = \"=\",   # Completion bar character\n                       incomplete = \"-\", # Incomplete bar character\n                       current = \"&gt;\",    # Current bar character\n                       clear = FALSE,    # If TRUE, clears the bar when finish\n                       width = 100)      # Width of the progress bar\n\nfor (s in 1:S){\n  pb$tick() #for progress bar\n  x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]\n  x.Ti        = x.Ti[p:1,]\n  for (i in 1:h){\n    x.T         = c(1,as.vector(t(x.Ti)))\n    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%posterior.ext.draws$A.posterior[,,s], sigma=posterior.ext.draws$Sigma.posterior[,,s])\n    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])\n  }\n}\n\n# plots of forecasts\nlibrary(plot3D)\nlibrary(MASS)\nlibrary(HDInterval)\npce.point.f    = apply(Y.h[,1,],1,mean) #one pce forecasts\npce.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)\npce.range      = range(y[,1],pce.interval.f)\n\nblue  = \"#05386B\"\nplum      =\"#BEBADA\"\nplum.rgb = col2rgb(\"thistle\")\nshade = rgb(plum.rgb[1],plum.rgb[2],plum.rgb[3],maxColorValue=255,alpha=100, names=\"thistle\")\n\n\n\npar(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)\nplot(1:(length(y[,1])+h),c(y[,1],pce.point.f), type=\"l\", ylim=pce.range, axes=FALSE, xlab=\"\", ylab=\"\", lwd=2, col=plum)\naxis(1,c(1,49,97,145,193, nrow(y),nrow(y)+h),c(\"2004-01\",\"2008-01\",\"2012-01\",\"2016-01\",\"2020-01\",\"\",\"\"), col=blue)\naxis(2,c(pce.range[1],mean(pce.range),pce.range[2]),c(\"\",\"PCE\",\"\"), col=blue)\nabline(v=nrow(y), col=\"gray42\")\ntext(x=228, y=9.635, srt=90, \"2023-02\")\nabline(v=nrow(y)+12, col=\"gray42\")\ntext(x=238, y=9.635, srt=90, \"2024-02\")\nabline(v=nrow(y)+24, col=\"gray42\")\ntext(x=250, y=9.635, srt=90, \"2025-02\")\npolygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[37:1]),\n        c(y[230,1],pce.interval.f[1,],pce.interval.f[2,36:1],y[230,1]),\n        col=shade, border=plum)\n\n\n\ndev.off()\n\nnull device \n          1"
  },
  {
    "objectID": "index.html#forecasting-with-the-second-extended-model",
    "href": "index.html#forecasting-with-the-second-extended-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Forecasting with the second extended model",
    "text": "Forecasting with the second extended model\n\n# simulate draws from the predictive density\nlibrary(mvtnorm)\nlibrary(progress)\nh = 36\nS = 27000\nY.h         = array(NA,c(h,N,S))\n\n#Progress bar\npb &lt;- progress_bar$new(format = \"(:spin) [:bar] :percent [Elapsed time: :elapsedfull || Estimated time remaining: :eta]\",\n                       total = S,\n                       complete = \"=\",   # Completion bar character\n                       incomplete = \"-\", # Incomplete bar character\n                       current = \"&gt;\",    # Current bar character\n                       clear = FALSE,    # If TRUE, clears the bar when finish\n                       width = 100)      # Width of the progress bar\n\nfor (s in 1:S){\n  pb$tick() #for progress bar\n  x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]\n  x.Ti        = x.Ti[p:1,]\n  for (i in 1:h){\n    x.T         = c(1,as.vector(t(x.Ti)))\n    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%posterior.sv.draws$A.posterior[,,s], sigma=posterior.sv.draws$Sigma.posterior[,,s])\n    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])\n  }\n}\n\n# plots of forecasts\nlibrary(plot3D)\nlibrary(MASS)\nlibrary(HDInterval)\n\npce.point.f    = apply(Y.h[,1,],1,mean) #one pce forecasts\npce.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)\npce.range      = range(y[,1],pce.interval.f)\n\nblue  = \"#05386B\"\nplum      =\"#BEBADA\"\nplum.rgb = col2rgb(\"thistle\")\nshade = rgb(plum.rgb[1],plum.rgb[2],plum.rgb[3],maxColorValue=255,alpha=100, names=\"thistle\")\n\n\n\npar(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)\nplot(1:(length(y[,1])+h),c(y[,1],pce.point.f), type=\"l\", ylim=pce.range, axes=FALSE, xlab=\"\", ylab=\"\", lwd=2, col=plum)\naxis(1,c(1,49,97,145,193, nrow(y),nrow(y)+h),c(\"2004-01\",\"2008-01\",\"2012-01\",\"2016-01\",\"2020-01\",\"\",\"\"), col=blue)\naxis(2,c(pce.range[1],mean(pce.range),pce.range[2]),c(\"\",\"PCE\",\"\"), col=blue)\nabline(v=nrow(y), col=\"gray42\")\ntext(x=228, y=9.52, srt=90, \"2023-02\")\nabline(v=nrow(y)+12, col=\"gray42\")\ntext(x=238, y=9.52, srt=90, \"2024-02\")\nabline(v=nrow(y)+24, col=\"gray42\")\ntext(x=250, y=9.52, srt=90, \"2025-02\")\npolygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[37:1]),\n        c(y[230,1],pce.interval.f[1,],pce.interval.f[2,36:1],y[230,1]),\n        col=shade, border=plum)\n\n\n\ndev.off()\n\nnull device \n          1 \n\n\nThe forecasts are built using the predictive density means and 90% highest density intervals. The 3 above plots present a sharp decline of personal consumption expenditures in the beginning of our forecasting horizon. A potential explanation could be that this drop is driven by the severe decline in the data caused by the COVID-19 crisis. The personal consumption expenditures is then predicted to continuously increase after March 2023. Moreover, we can see that the confidence interval increases for forecasts with longer time horizons. In 3 years, personal consumption expenditures are expected to be slightly lower than the current level. However, considering the width of the confidence interval, this result is quite uncertain."
  },
  {
    "objectID": "index.html#set-up-of-the-model-proof",
    "href": "index.html#set-up-of-the-model-proof",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Set up of the model proof",
    "text": "Set up of the model proof\nLet’s generate artificial data simulated from a bi-variate Gaussian random walk process with covariance matrix that is an identity matrix of order 2. To estimate the model, we consider one lag with the artificial data.\n\n### Specify the setup\np = 1\nN = 2\nK = 1+N*p\nS = c(5000,50000)\n\n### Generate RW data process\nrw.1    = cumsum(rnorm(1000,0,1))\nrw.2    = cumsum(rnorm(1000,0,1))\ny       = matrix(cbind(rw.1,rw.2),nrow=1000,ncol=N)\n\n### Create Y and X matrices\nY       = ts(y[2:nrow(y),])\nX       = matrix(1,nrow(Y),1)\nX       = cbind(X,y[1:nrow(y)-p,])"
  },
  {
    "objectID": "index.html#proof-of-baseline-model",
    "href": "index.html#proof-of-baseline-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Proof of baseline model",
    "text": "Proof of baseline model\n\n### MLE\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n### Specify the priors (Minnesota prior)\nkappa.1           = 0.02^2\nkappa.2           = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\n\npriors = list(\n  A.prior     = A.prior,\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior     = diag(diag(Sigma.hat)),\n  nu.prior    = N+1 \n)\n\n## Apply function BVAR\nposterior.draws = BVAR(Y=Y, X=X, priors=priors, S=S)\nround(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)\n\n       [,1]   [,2]\n[1,]  1.001 -0.028\n[2,] -0.028  0.954\n\nround(apply(posterior.draws$A.posterior, 1:2, mean),3)\n\n       [,1]   [,2]\n[1,] -0.015  0.167\n[2,]  0.990 -0.006\n[3,]  0.002  0.991\n\n\nAs the obtained posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and the posterior mean of the constant term is close to a vector containing only zeros, we can conclude that the model is correct. ## Proof of extended model\n\n### Proof of extension model\n\n### MLE\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n### Modify the priors\nkappa.1           = 1\nkappa.2           = 10\ninitial_kappa     = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\n\npriors = list(\n  A.prior            = A.prior,\n  V.prior            = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior            = diag(diag(Sigma.hat)),\n  nu.prior           = N+1,\n  s.kappa.prior      = 2,\n  nu.kappa.prior     = 4\n)\n\n## Apply function BVAR_extension\nposterior.draws = BVAR_extension(Y=Y, X=X, priors=priors, initial_kappa=initial_kappa, S=S)\nround(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)\n\n       [,1]   [,2]\n[1,]  1.001 -0.028\n[2,] -0.028  0.954\n\nround(apply(posterior.draws$A.posterior, 1:2, mean),3)\n\n       [,1]   [,2]\n[1,] -0.014  0.174\n[2,]  0.990 -0.007\n[3,]  0.002  0.991\n\n\nThe results from the BVAR_extension function show that the correctness of the model."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "References",
    "text": "References\nAprigliano, V., Ardizzi, G., & Monteforte, L. (2019), “Using Payment System Data to Forecast Economic Activity,” International Journal of Central Banking, International Journal of Central Banking, vol. 15(4), pages 55-80, October.\nBaker, S. R., Bloom, N., & Davis, S. J. (2016). Measuring Economic Policy Uncertainty. The Quarterly Journal of Economics, 131(4), 1593–1636. https://doi.org/10.1093/qje/qjw024\nCarlsen, M. & Storgaard, P. E. (2010), “Dankort Payments as a Timely Indicator of Retail Sales in Denmark.” Danmarks Nationalbank Working Papers n°66.\nEllingsen, J., Larsen, V. H., & Thorsrud, L. A. (2021). News media versus FRED‐MD for macroeconomic forecasting. Journal of Applied Econometrics, 37(1), 63 – 81. https://doi.org/10.1002/jae.2859.\nEsteves, P. S. (2009), “Are ATM/POS Data Relevant When Nowcasting Private Consumption?”, Working Papers n°25, Banco de Portugal.\nGalbraith, J. W., & Tkacz, G. (2013). Nowcasting GDP: Electronic Payments, Data Vintages and the Timing of Data Releases. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2342228.\nGil, M., Perez, J. J., Sanchez Fuentes, A. J., & Urtasun, A. (2018). “Nowcasting Private Consumption: Traditional Indicators, Uncertainty Measures, Credit Cards and Some Internet Data”, Working Paper No. 1842, Banco de Espana.\nLazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). The Parable of Google Flu: Traps in Big Data Analysis. Science (New York, N.Y.), 343, 1203–1205. https://doi.org/10.1126/science.1248506\nSchmidt, T., & Vosen, S. (2009). Forecasting Private Consumption: Survey-Based Indicators vs. Google Trends. Journal of Forecasting, 30. https://doi.org/10.2139/ssrn.1514369\nWoo, J., & Owen, A. (2018). Forecasting Private Consumption with Google Trends Data. Journal of Forecasting, 38. https://doi.org/10.1002/for.2559"
  }
]