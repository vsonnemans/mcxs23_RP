---
title: "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data"
author: "Victoria Sonnemans"

execute:
  echo: false
  
bibliography: references.bib
---

```{css, echo = FALSE}
.justify {
text-align: justify !important
}
```

::: {.justify}
> **Abstract.** Can Bayesian VARs and alternative data help better estimate the future state of household final consumption expenditures in Australia? Which combination of traditional and alternative data provides the highest accuracy in the latter indicator? As household consumption is a major component of GDP, this research project will study the potential use of Bayesian VARs with a mix of traditional economic data such as disposable income and an alternative data source which is Google Trends to forecast this indicator.

> **Keywords.** Bayesian Vars, Google Trends, new data types

# The question, objective and motivation

## Research question

Can Bayesian VARs and alternative data help better estimate the future state of household final consumption expenditures in the US? Which combination of traditional and alternative data provides the highest accuracy in the latter indicator?

## Objective and Motivation

As it drives around 50% of the US GDP, Personal Consumption Expenditures (PCE) is a leading indicator to gauge the economic health of a country. There is thus a high incentive to improve the accuracy of its predictions. This has encouraged researchers to investigate big data as alternative sources. For instance, Schimdt and Vosen (2009) use search query time-series provided by Google Trends to forecast consumption. Esteves (2009), Aprigliano, et al. (2019), Galbraith and Tkacz (2013), Carlsen and Storgaard (2010) analyze electronic payments to predict consumption as these can track a large percentage of spending activities. Ellingsen, et al. (2021) demonstrate that news media data capture information about consumption that hard economic indicators do not. Gil et al. (2018) investigate the potential of the Economic Policy Uncertainty index derived from news data and developed by Baker et al. (2016) to predict consumption.

According to Professor Tomasz Wozniak from the University of Melbourne, forecasting with Bayesian VARs often leads to more precise forecasts than when using the frequentist approach to forecasting because of the effect of the priors. Despite the benefits of Bayesian VARs, there is nearly no research on the combination of Bayesian VARs and alternative data. Existing articles either investigate the use of Bayesian estimation models or of alternative data to forecast indicators but do not consider both together.

This paper will compare the forecasts of PCE in Australia from Bayesian VARs and several extensions applied on different sets of variables. These datasets will include both traditional macroeconomic variables computed by statistical offices and alternative data such as Google Trends. This research project contributes to the literature by studying PCE, an indicator that has to date received scant attention from the Bayesian VARs literature. Moreover, it proposes the first investigation of the combination of Bayesian VARs with alternative data to forecast PCE.

# The data and their properties

To forecast PCE, we will construct a dataset which contains two types of variables: traditional macroeconomic indicators and alternative variables.

The traditional variables included in this analysis were collected from the readrba package and will be:

-   Real personal consumption expenditures

-   Real personal disposable income

-   Consumer Price Index 

-   Consumer Sentiment (University of Michigan)

-   30-Year Fixed Rate Mortgage

-   Unemployment rate

-   Home price index

-   Google Trends series for PCE of durable goods

-   Google Trends series for PCE of non-durable goods

-   Google Trends series for PCE of services

Incorporating traditional economic variables along alternative data in this exercise is important as we do not want to fall in the "Big Data hubris" trap introduced by (Lazer et al., 2014). The "Big Data Hubris" is the problematic assumption that alternative data or big data can be used as a replacement for traditional data rather than a supplement. To prevent this, combining non-standard and standard variables together can lead to greater value. Moreover, previous extensive literature already has recognized the predictive power of these macroeconomic variables. For instance, previous research have identified co-movements of most common consumer sentiment indicators and real PCE (Schimdt and Vosen, 2009). 

##### Figure 1. Time series plots of the traditional variables {style="text-align: center;"}

```{r setup, echo = F, warning=FALSE, message = FALSE}
library(xts)
library(tseries)
library(ggplot2)
library(patchwork)
library(dygraphs)
library(timeDate)
library(zoo)
library(RColorBrewer)
library(openxlsx)
library(readr)
library(knitr)
library(readxl)
library(httr)
library(rmarkdown)

library(fredr)

fredr_set_key("f1d6de070cd07cd6028872a3bc573657")

#Real personal consumption expenditures (monthly)
pce.dl<- fredr(series_id = "PCEC96", observation_start = as.Date("2004-01-01"), observation_end = as.Date("2023-02-01"))
pce<-xts(pce.dl$value, pce.dl$date)


#Real household disposable income (monthly)
inc.dl<-fredr(series_id = "DSPIC96", observation_start = as.Date("2004-01-01"), observation_end = as.Date("2023-02-01"))
inc<-xts(inc.dl$value, inc.dl$date)
         

#Inflation-Consumer Price Index (monthly)
cpi.dl<-fredr(series_id = "USACPIALLMINMEI", observation_start = as.Date("2004-01-01"), observation_end = as.Date("2023-02-01"))
cpi<-xts(cpi.dl$value, cpi.dl$date)
#cpi <- 100*diff(temp_cpi)/temp_cpi


#University of Michigan Consumer Sentiment UMCSENT (monthly)
consum_sent.dl<-fredr(series_id = "UMCSENT", observation_start = as.Date("2004-01-01"), observation_end = as.Date("2023-02-01"))
consum_sent<-xts(consum_sent.dl$value, consum_sent.dl$date)

#30-Year Fixed Rate Mortgage Average (weekly)
mortgage_rate.daily <-fredr(series_id = "MORTGAGE30US", observation_start = as.Date("2004-01-01"), observation_end = as.Date("2023-02-02"))
mortgage_rate_monthly <-to.monthly(xts(mortgage_rate.daily$value, mortgage_rate.daily$date))
mortgage_rate <- mortgage_rate_monthly[,1] #take open value

#Unemployment rate (monthly)
unemp_rate.dl<-fredr(series_id = "UNRATE", observation_start = as.Date("2004-01-01"), observation_end = as.Date("2023-02-01"))
unemp_rate<- xts(unemp_rate.dl$value, unemp_rate.dl$date)

#Home price index (monthly)
home_price.dl<-fredr(series_id = "CSUSHPISA", observation_start = as.Date("2004-01-01"), observation_end = as.Date("2023-02-01"))
home_price<- xts(home_price.dl$value, home_price.dl$date)


#Google Trends series for pce of durable goods (monthly)
GT_dur_g <- read.xlsx("durable_goods.xlsx")


#Google Trends series for pce of non-durable goods (monthly)
GT_ndur_g <- read.xlsx("GT for PCE of non-durable goods.xlsx")

#Google Trends series for pce of non-durable goods (monthly)
GT_serv <- read.xlsx("GT for PCE of services.xlsx")

#Merge into one dataframe
Y.df  <- data.frame(pce, inc ,cpi,  consum_sent, mortgage_rate, unemp_rate, home_price)

varname_vec <- c("PCE", "Real disposable income","CPI", "Consumer Confidence Index","Mortgage rate","Unemployment rate", "Home price index")
colnames(Y.df) <- varname_vec
```

------------------------------------------------------------------------

As for the alternative data, a Google Trends index will be included. Google Trends is a tool which reveals how frequently a certain keyword has been searched for on the Google web browser. The platform provides weekly aggregated time series starting in 2004 for a specific keyword that can be directly downloaded using the R package "gtrendsR". The Google Trends index will be constructed based on the methodology presented in Schmidt and Vosen (2009). However, we will adapt their approach to match Australian specifies. Therefore, the specific keywords to construct the index have yet to be chosen. Moreover, Google Trends have been quite extensively studied to forecast labor market indicators but only a few articles have focused on PCE.

##### Figure 1. Time series plots of the alternative variables {style="text-align: center;"}

## Preliminary data analysis

### Augmented Dickey-Fuller test for unit roots of traditional variables
##### Table 1. ADF results for traditional log transformed variables {style="text-align: center;"}
```{r ADF on log data, echo = F, warning=FALSE, message = FALSE}
##### on log data
# Transform into natural logs
df.log <- data.frame(log(pce),
                     log(inc),
                     log(cpi),
                     log(consum_sent),
                     (mortgage_rate),
                     (unemp_rate),
                     log(home_price))

colnames(df.log) <- c("PCE", "Real disposable income","CPI", "Consumer Confidence Index","Mortgage rate","Unemployment rate", "Home price index")

adf.log.result <- as.data.frame(matrix(nrow=7,ncol=6,NA))
varname_adf <- c("Variable", "Statistic", "parameter", "alternative", "p.value", "result")
colnames(adf.log.result) <- varname_adf

for (i in 1:ncol(Y.df)){
  adf <- adf.test(Y.df[,i])
  adf.log.result[i,1] <- varname_vec[i]
  adf.log.result[i,2] <- adf[1]
  adf.log.result[i,3] <- adf[2]
  adf.log.result[i,4] <- adf[3]
  adf.log.result[i,5] <- adf[4]
  if (adf[4]<=0.05){
    adf.log.result[i,6] <- "Stationary"  
  } else {
    adf.log.result[i,6] <- "Non-stationary"
  }
}

paged_table(as.data.frame(adf.log.result))

```

##### Table 2. ADF results for first difference of traditional variables {style="text-align: center;"}
```{r ADF on first diff data, echo = F, warning=FALSE, message = FALSE}
#on first difference
adf.first.result <- as.data.frame(matrix(nrow=7,ncol=6,NA))
varname_adf <- c("Variable", "Statistic", "parameter", "alternative", "p.value", "result")
colnames(adf.first.result) <- varname_adf

for (i in 1:ncol(Y.df)){
  adf <- adf.test(diff(Y.df[,i]))
  adf.first.result[i,1] <- varname_vec[i]
  adf.first.result[i,2] <- adf[1]
  adf.first.result[i,3] <- adf[2]
  adf.first.result[i,4] <- adf[3]
  adf.first.result[i,5] <- adf[4]
  if (adf[4]<=0.05){
    adf.first.result[i,6] <- "Stationary"  
  } else {
    adf.first.result[i,6] <- "Non-stationary"
  }
}

paged_table(as.data.frame(adf.first.result))
```
##### Table 3. ADF results for the second difference of Home price index {style="text-align: center;"}
```{r ADF on second diff data, echo = F, warning=FALSE, message = FALSE}
#on second difference of home price index
adf.sec.result <- as.data.frame(matrix(nrow=1,ncol=6,NA))
varname_adf <- c("Variable", "Statistic", "parameter", "alternative", "p.value", "result")
colnames(adf.sec.result) <- varname_adf

adf <- adf.test(diff(diff(Y.df[,"Home price index"])))
adf.sec.result[1,1] <- "Home Price index"
adf.sec.result[1,2] <- adf[1]
adf.sec.result[1,3] <- adf[2]
adf.sec.result[1,4] <- adf[3]
adf.sec.result[1,5] <- adf[4]
if (adf[4]<=0.05){
    adf.sec.result[1,6] <- "Stationary"  
} else {
    adf.sec.result[1,6] <- "Non-stationary"
}

#Apply transformations
df.log <- data.frame(log(pce),
                     log(inc),
                     diff(cpi),
                     log(consum_sent),
                     mortgage_rate,
                     unemp_rate,
                     log(home_price))

colnames(df.log) <- c("PCE", "Real disposable income","CPI", "Consumer Confidence Index","Mortgage rate","Unemployment rate", "Home price index")



paged_table(as.data.frame(adf.sec.result))

```
### Augmented Dickey-Fuller test for unit roots of alternative variables
##### Table 4. ADF results for the first difference of GT for PCE of durable goods
```{r ADF on first diff of durable goods GT, echo = F, warning=FALSE, message = FALSE}
# GT durable goods
adf.dur.result <- as.data.frame(matrix(nrow=22,ncol=6,NA))
varname_adf <- c("Variable", "Statistic", "parameter", "alternative", "p.value", "result")
colnames(adf.dur.result) <- varname_adf
GT_dur_g_adf <- GT_dur_g[,3:24]
colnames <- colnames(GT_dur_g_adf)

for (i in 1:ncol(GT_dur_g_adf)){
  adf <- adf.test(diff(GT_dur_g_adf[,i]))
  adf.dur.result[i,1] <- colnames[i]
  adf.dur.result[i,2] <- adf[1]
  adf.dur.result[i,3] <- adf[2]
  adf.dur.result[i,4] <- adf[3]
  adf.dur.result[i,5] <- adf[4]
  if (adf[4]<=0.05){
    adf.dur.result[i,6] <- "Stationary"  
  } else {
    adf.dur.result[i,6] <- "Non-stationary"
  }
}

paged_table(as.data.frame(adf.sec.result))

```
##### Table 5. ADF results for the first difference of GT for PCE of non-durable goods
```{r ADF on first diff of non_durable goods GT, echo = F, warning=FALSE, message = FALSE}
# GT durable goods
adf.dur.result <- as.data.frame(matrix(nrow=22,ncol=6,NA))
varname_adf <- c("Variable", "Statistic", "parameter", "alternative", "p.value", "result")
colnames(adf.dur.result) <- varname_adf
GT_dur_g_adf <- GT_dur_g[,3:24]
colnames <- colnames(GT_dur_g_adf)

for (i in 1:ncol(GT_dur_g_adf)){
  adf <- adf.test(diff(GT_dur_g_adf[,i]))
  adf.dur.result[i,1] <- colnames[i]
  adf.dur.result[i,2] <- adf[1]
  adf.dur.result[i,3] <- adf[2]
  adf.dur.result[i,4] <- adf[3]
  adf.dur.result[i,5] <- adf[4]
  if (adf[4]<=0.05){
    adf.dur.result[i,6] <- "Stationary"  
  } else {
    adf.dur.result[i,6] <- "Non-stationary"
  }
}

paged_table(as.data.frame(adf.sec.result))

```
##### Table 6. ADF results for the first difference of GT for PCE of services
```{r ADF on first diff of services GT, echo = F, warning=FALSE, message = FALSE}
# GT durable goods
adf.dur.result <- as.data.frame(matrix(nrow=22,ncol=6,NA))
varname_adf <- c("Variable", "Statistic", "parameter", "alternative", "p.value", "result")
colnames(adf.dur.result) <- varname_adf
GT_dur_g_adf <- GT_dur_g[,3:24]
colnames <- colnames(GT_dur_g_adf)

for (i in 1:ncol(GT_dur_g_adf)){
  adf <- adf.test(diff(GT_dur_g_adf[,i]))
  adf.dur.result[i,1] <- colnames[i]
  adf.dur.result[i,2] <- adf[1]
  adf.dur.result[i,3] <- adf[2]
  adf.dur.result[i,4] <- adf[3]
  adf.dur.result[i,5] <- adf[4]
  if (adf[4]<=0.05){
    adf.dur.result[i,6] <- "Stationary"  
  } else {
    adf.dur.result[i,6] <- "Non-stationary"
  }
}

paged_table(as.data.frame(adf.sec.result))

```
# The Modelling Framework 

## The Baseline Model

The model used for the forecasting experiment is a VAR(p) model:

```{=tex}
\begin{aligned}
& y_t  =\mu_0+A_1 y_{t-1}+\cdots+A_p y_{t-p}+\epsilon_t \\ & \epsilon_t \mid  Y_{t-1}  \sim i i d \mathcal{N}_N\left(\mathbf{0}_N, \Sigma\right)
\end{aligned}
```

Where $N=11$ and $y_t$ is the vector of 11 variables:

$$
y_t=\left(\begin{array}{cc}\operatorname{pce}_t & =\text {Real PCE }  \\\operatorname{inc}_t & =\text { Real household disposable income } \\\text { cpi }_t & =\text { Consumer price index } \\\text { consum_sent}_t & =\text { Consumer sentiment indicator }\\\text { mortgage_rate}_t & =\text { Mortgage rate }\\\text { unemp_rate}_t & =\text { Unemployment rate }\\\text { home_price}_t & =\text { Home price index } \\\text{gt_dur_goods}_t & =\text { Google Trends index for PCE of durable goods }
\\\text{gt_ndur_goods}_t & =\text { Google Trends index for PCE of non durable goods }
\\\text{gt_services}_t & =\text { Google Trends index for PCE of services }
\end{array}\right)
$$

The model can also be written in matrix notation:

```{=tex}
\begin{aligned}
Y & =X A+E \\E \mid X & \sim \mathcal{M N} _{T \times N}\left(\mathbf{0}_{T \times N}, \Sigma, I_T\right)
\end{aligned}
```

Where $Y$ is a $T\times11$ matrix, $X$ is a $T\times(1+(11\times p))$, $A$ is a $(1+(11\times p))\times 11$ matrix that contains the relationships between the variables and $E$ is a $T\times11$. T and p are yet to be determined during the continuation of this research project.

### The Likelihood function

$$
\begin{aligned}
Y \mid X,A,\Sigma & \sim \mathcal{M N} _{T \times N}\left(XA, \Sigma, I_T\right)
\end{aligned}
$$
$$
\begin{aligned}
L(A, \Sigma \mid Y, X) & \propto det(\Sigma)^{-\frac{T}{2}} \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(Y-X A)^{\prime}(Y-X A)\right]\right\} \\
&= det (\Sigma)^{-\frac{T}{2}} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(A-\widehat{A})^{\prime} X^{\prime} X(A-\widehat{A})\right]\right\} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(Y-X \widehat{A})^{\prime}(Y-X \widehat{A})\right]\right\}
\end{aligned}
$$

### The prior distributions
$$
\begin{aligned}
&A \mid \Sigma \sim  \mathcal{M N} _{T \times N}(\underline{A}, \Sigma, \underline{V}) \\
&\Sigma \sim \mathcal{IW}_{N}(\underline{S}, \underline{\nu})
\end{aligned}
$$
$$
\begin{aligned}
p(A, \Sigma) \propto & \operatorname{det}(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(A-\underline{A})^{\prime} \underline{V}^{-1}(A-\underline{A})\right]\right\} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1} \underline{S}\right]\right\}
\end{aligned}
$$
In the code, we set:
A as 
V as
S as 
Nu as 

### The posterior distribution
#### The Derivations of the posterior distribution

$$
\begin{aligned}
p(A,\Sigma \mid Y,X) \propto L(A, \Sigma \mid Y,X) p(A,\Sigma) = L(A, \Sigma \mid Y,X) p(A\mid\Sigma)p(\Sigma)
\end{aligned}
$$

Let's focus on the kernel

$$
\begin{aligned}
p(A,\Sigma \mid Y,X) \propto & det (\Sigma)^{-\frac{T}{2}} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(A-\widehat{A})^{\prime} X^{\prime} X(A-\widehat{A})\right]\right\} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(Y-X \widehat{A})^{\prime}(Y-X \widehat{A})\right]\right\} \\
& \times det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(A-\underline{A})^{\prime} \underline{V}^{-1}(A-\underline{A})\right]\right\} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1} \underline{S}\right]\right\} \\

& = \operatorname{det}(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma ^ { - 1 } 

[\color{green}{(A-\widehat{A})^{\prime} X^{\prime} X(A-\widehat{A})+(A-\underline{A})^{\prime} \underline{V}^{-1}(A-\underline{A})}\right\} \\
&\color{green}{\left.\left.+(Y-X \widehat{A})^{\prime}(Y-X \widehat{A})+\underline{S}\right]\right]\right\}}
\end{aligned}
$$

We can now complete the squares for the green part of the equation.

```{=tex}
\begin{aligned}
& 
\color{blue}{(A-\widehat{A})^{\prime} X^{\prime} X(A-\widehat{A})}
\color{green}{
+(A-\underline{A})^{\prime} \underline{V}^{-1}(A-\underline{A})}
\color{red}{+(Y-X \widehat{A})^{\prime}(Y-X \widehat{A})}
\color{black}{+\underline{S}} \\

& \color{blue}{=A^\prime X^\prime XA -A^\prime X^\prime X \widehat{A} - \widehat{A}^\prime X^\prime XA + \widehat{A}^\prime X^\prime X \widehat{A}}
\color{green}{ +A^\prime \underline{V}^{-1}A - A^\prime \underline{V}^{-1} \underline{A} - \underline{A}^\prime \underline{V}^{-1} A + \underline{A}^\prime \underline{V}^{-1} \underline{A}}
\color{red}{+ Y^\prime Y-Y^\prime X \widehat{A} - \widehat{A}^\prime X^\prime Y + \widehat{A}^\prime X^\prime X \widehat{A}}
\color{black}{+ \underline{S}}\\
& \color{blue}{=A^\prime X^\prime XA -Y^\prime XA - \widehat{A}^\prime X^\prime XA}
\color{green}{ +A^\prime \underline{V}^{-1}A - A^\prime \underline{V}^{-1} \underline{A} - \underline{A}^\prime \underline{V}^{-1} A}
\color{red}{+ Y^\prime Y} 
\color{black}{+\underline{S}}
\color{green}{+ \underline{A}^\prime \underline{V}^{-1} \underline{A}}\\
& = A^\prime (X^\prime X+ \underline{V}^{-1})A -2A^\prime (X^\prime Y+\underline{V}^{-1} \underline{A}) + Y^\prime Y+ \underline{S} + \underline{A}^\prime \underline{V}^{-1} \underline{A}
\end{aligned}
```

We can set  $\overline{V}^{-1}=X^\prime X+ \underline{V}^{-1}$ 


$$
\begin{aligned}
& = A^\prime \overline{V}^{-1}A -2A^\prime \overline{V}^{-1}\overline{V} (X^\prime Y+\underline{V}^{-1} \underline{A}) + Y^\prime Y+ \underline{S} + \underline{A}^\prime \underline{V}^{-1} \underline{A}
\end{aligned}
$$

We can set $\overline{A}=\overline{V}(X^\prime Y+\underline{V}^{-1}\underline{A})$

$$
\begin{aligned}
& = A^\prime \overline{V}^{-1}A -2A^\prime \overline{V}^{-1}\overline{A} \pm \overline{A}^\prime \overline{V}^{-1}\overline{A} + Y^\prime Y+ \underline{S} + \underline{A}^\prime \underline{V}^{-1} \underline{A}\\
& =(A-\overline{A})^\prime \overline{V}^{-1}(A-\overline{A})-\overline{A}^\prime \overline{V}^{-1}\overline{A}+Y^\prime Y+\underline{S}+\underline{A}^\prime \underline{V}^{-1} \underline{A}
\end{aligned}
$$

Let's put the latter expression back in the $exp$.

$$
\begin{aligned}
\exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(A-\overline{A})^{\prime} \overline{V}^{-1}(A-\overline{A})\right]\right\}\exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}\overline{S}\right]\right\}
\end{aligned}
$$

$$
\boxed{
\begin{array}{rcl}
&p(A,\Sigma \mid Y,X)= p(A \mid Y,X,\Sigma)p(\Sigma \mid Y,X) \\
&p(A \mid Y,X,\Sigma)=\mathcal{M N}_{K\times N}(\overline{A},\Sigma,\overline{V})\\
&p(\Sigma \mid Y,X)=\mathcal{I W}_N (\overline{S},\overline{\nu})\\
& \overline{V} = (X^\prime X + \underline{V}^{-1})^{-1}\\
& \overline{A}=\overline{V}(X^\prime Y+\underline{V}^{-1}\underline{A})\\
& \overline{\nu}=T+\underline{\nu}\\
& \overline{S}=\underline{S}+Y^\prime Y+\underline{A}^\prime \underline{V}^{-1}\underline{A}-\overline{A}^\prime \overline{V}^{-1}\overline{A}
\end{array}
}
$$

### The Code for Bayesian VAR estimation

```{r function based on basic model}
#| echo: true
#| message: false
#| warning: false

# Bayesian estimation of the baseline model

## Specify the setup
N       = ncol(df.log)
p       = 4
K       = 1+N*p
S       = c(5000,100000)
h       = 20
set.seed(123456)

## Create Y and X matrices
y       = ts(df.log, start=c(2000,1), frequency=4)
Y       = ts(y[5:nrow(y),], start=c(2001,1), frequency=4)
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[5:nrow(y)-i,])
}

T       = nrow(Y)
## MLE
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T

## Specify the priors (Minnesota prior)
kappa.1           = 0.02^2
kappa.2           = 100
A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)

priors = list(
  A.prior     = A.prior,
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),
  S.prior     = diag(diag(Sigma.hat)),
  nu.prior    = N+1 
)

## BVAR function

BVAR = function(Y,X,priors,S){
  
  # normal-inverse Wishard posterior parameters
  V.bar.inv   = t(X)%*%X + diag(1/diag(priors$V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(priors$V.prior))%*%priors$A.prior)
  nu.bar      = nrow(Y) + priors$nu.prior
  S.bar       = priors$S.prior + t(Y)%*%Y + t(priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  #posterior draws
  Sigma.posterior   = rWishart(sum(S), df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,sum(S)))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),sum(S)))),c(dim(A.bar),sum(S)))
  L                 = t(chol(V.bar))
  
  for (s in 1:sum(S)){
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
  }
  
  posterior = list(
    Sigma.posterior   = Sigma.posterior,
    A.posterior       = A.posterior
  )
  return(posterior)
}


## Apply function BVAR
posterior.draws = BVAR(Y=Y, X=X, priors=priors, S=S)
```

The output of the BVAR function applied on the baseline model is: 
```{r ouptput}
#| echo: true
#| message: false
#| warning: false
round(apply(posterior.draws$A.posterior, 1:2, mean),3)
```

```{r output 2}
#| echo: true
#| message: false
#| warning: false
round(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)
```

## The extended model
### The prior distribution
$$
\begin{array}{rcl}
&A \mid \Sigma,{\color{red}\kappa} \sim  \mathcal{M N} _{T \times N}(\underline{A}, \Sigma, {\color{red}\kappa}\underline{V}) \\
&\color{red}{\kappa \sim \mathcal{IG2}(\underline{s}_\kappa,\underline{\nu}_\kappa)}\\
&\Sigma \sim \mathcal{IW}_{N}(\underline{S}, \underline{\nu})
\end{array}
$$

### The posterior distribution
In this section, we will derive the the joint full-conditional posterior distribution of $A$ and $\Sigma$ and the full-conditional posterior distribution of $\kappa$.

#### The derivations of the joint full-conditional posterior distribution of A and Sigma
$$
\begin{aligned}
p(A,\Sigma \mid Y,X,{\color{red}\kappa}) \propto L(A, \Sigma \mid Y,X) p(A,\Sigma) = L(A, \Sigma \mid Y,X) p(A\mid\Sigma,{\color{red}\kappa})p(\Sigma)p({\color{red}\kappa})
\end{aligned} 4
$$
Let's focus on the kernel

```{=tex}
\begin{aligned}
p(A,\Sigma \mid Y,X,{\color{red}{\kappa}}) \propto & det (\Sigma)^{-\frac{T}{2}} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(A-\widehat{A})^{\prime} X^{\prime} X(A-\widehat{A})\right]\right\} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(Y-X \widehat{A})^{\prime}(Y-X \widehat{A})\right]\right\} \\
& \times det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(A-\underline{A})^{\prime} \underline{V}^{-1}(A-\underline{A})\right]\right\} \\
& \times \exp \left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1} \underline{S}\right]\right\}\\

& = \operatorname{det}(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \\
& \times \exp \{-\frac{1}{2} \operatorname{tr}[\Sigma^{-1} [(A-\widehat{A})^{\prime} X^{\prime} X(A-\widehat{A})+(A-\underline{A})^{\prime} \frac{1}{\kappa} \underline{V}^{-1}(A-\underline{A}) \\
& +(Y-X \widehat{A})^{\prime}(Y-X \widehat{A})+\underline{S}]]\}

\end{aligned}
```


```{=tex}
\begin{aligned}
& 
(A-\widehat{A})^{\prime} X^{\prime} X(A-\widehat{A})
+(A-\underline{A})^{\prime} {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1}(A-\underline{A})
+(Y-X \widehat{A})^{\prime}(Y-X \widehat{A})
+\underline{S} \\

& =A^\prime X^\prime XA -A^\prime X^\prime X \widehat{A} - \widehat{A}^\prime X^\prime XA + \widehat{A}^\prime X^\prime X \widehat{A}
 +A^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1}A - A^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1} \underline{A} - \underline{A}^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1} A + \underline{A}^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1} \underline{A}
+ Y^\prime Y-Y^\prime X \widehat{A} - \widehat{A}^\prime X^\prime Y + \widehat{A}^\prime X^\prime X \widehat{A}+ \underline{S}\\

& =A^\prime X^\prime XA -Y^\prime XA - \widehat{A}^\prime X^\prime XA
 +A^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1}A - A^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1} \underline{A} - \underline{A}^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1} A
+ Y^\prime Y+ \underline{S} + \underline{A}^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1} \underline{A}\\
& = A^\prime (X^\prime X+ {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1})A -2A^\prime (X^\prime Y+{\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1} \underline{A}) + Y^\prime Y+ \underline{S} + \underline{A}^\prime {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1} \underline{A}
\end{aligned}
```

```{=tex}
\begin{aligned}
\boxed{
\begin{array}{rcl}
&p(A,\Sigma \mid Y,X,{\color{red}\kappa})= \mathcal{MNIW}_{K\times N}(\overline{A},\overline{V},\overline{S},\overline{\nu}) \\
& \overline{V} = (X^\prime X + {\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1})^{-1}\\
& \overline{A}=\overline{V}(X^\prime Y+{\color{red}{\frac{1}{\kappa}}}\underline{V}^{-1}\underline{A})\\
& \overline{\nu}=T+\underline{\nu}\\
& \overline{S}=\underline{S}+Y^\prime Y+\underline{A}^\prime \underline{V}^{-1}{\color{red}{\frac{1}{\kappa}}}\underline{A}-\overline{A}^\prime \overline{V}^{-1}\overline{A}
\end{array}
}
\end{aligned}
```

#### The Derivations of the full-conditional posterior distribution of $\kappa$

```{=tex}
\begin{align}
p(\kappa \mid A, \Sigma, Y,X) & =L(A,\Sigma \mid Y,X)p(\kappa \mid \underline{s}_{\kappa}, \underline{\nu}_{\kappa})p(A,\Sigma)\\
& =L(A,\Sigma \mid Y,X)p(\kappa \mid \underline{s}_{\kappa}, \underline{\nu}_{\kappa})p(A\mid \Sigma)p(\Sigma)\\

&= (\kappa)^{-\frac{\underline{\nu}+2}{2}}\exp\left\{\frac{-1}{2}\frac{\underline{s}_{\kappa}}{\kappa}\right\} \\
& \times det(\kappa \underline{V})^{-\frac{N}{2}}
\exp\left\{-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1}(A-\underline{A})^{\prime} \frac{1}{\kappa}\underline{V}^{-1}(A-\underline{A})\right]\right\} \\

&= (\kappa)^{-\frac{\underline{\nu}+2}{2}}(\kappa)^{-\frac{KN}{2}}\kappa ^{-\frac{\underline{\nu}+KN+2}{2}}\times \exp\left\{\frac{-1}{2} \frac{1}{\kappa}(\underline{s}_{\kappa}+\operatorname{tr}\left[\Sigma^{-1}(A-\underline{A})^{\prime} \underline{V}^{-1}(A-\underline{A})\right])\overline{s}_{\kappa}\right\}\\

&= \kappa ^{-\frac{\overbrace{\underline{\nu}+KN}^{\overline{\nu}_{\kappa}}+2}{2}}\times \exp\left\{\frac{-1}{2} \frac{1}{\kappa}\underbrace{(\underline{s}_{\kappa}+\operatorname{tr}\left[\Sigma^{-1}(A-\underline{A})^{\prime} \underline{V}^{-1}(A-\underline{A})\right])}_{\overline{s}_{\kappa}}\right\}

\end{align}
```

We obtain the following full-conditional posterior distribution of $\kappa$:

$$
\boxed{
\begin{array}{rcl}
&\kappa \sim \mathcal{IG2}(\overline{s}_{\kappa},\overline{\nu}_{\kappa})\\
& \overline{s}_{\kappa}=\underline{s}_{\kappa}+\operatorname{tr}\left[\Sigma^{-1}(A-\underline{A})^{\prime} \underline{V}^{-1}(A-\underline{A})\right]\\
&\overline{\nu}_{\kappa}=\underline{\nu}+KN
\end{array}
}
$$

### The code for Bayesian VAR estimation
Let's adapt the BVAR function to the extended model. 

```{r extension model}
#| echo: true
#| message: false
#| warning: false
#Extended model

## Modify the priors
kappa.1           = 1
kappa.2           = 10
initial_kappa     = 100
A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)

priors = list(
  A.prior            = A.prior,
  V.prior            = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),
  S.prior            = diag(diag(Sigma.hat)),
  nu.prior           = N+1,
  s.kappa.prior      = 2,
  nu.kappa.prior     = 4
)

## Modify BVAR function

BVAR_extension = function(X,Y,priors,initial_kappa,S){
  
  A.posterior        = array(NA, dim = c(K,N,sum(S)))
  Sigma.posterior    = array(NA,dim=c(N,N,sum(S)))
  kappa.posterior    = matrix(NA, sum(S), 1) 
  kappa.posterior[1] = initial_kappa
  
  for (s in 1:sum(S)){
    
    # full-cond of joint posterior of A and Sigma
    V.bar.inv   = t(X)%*%X + diag(1/diag(kappa.posterior[s]*priors$V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(kappa.posterior[s]*priors$V.prior))%*%priors$A.prior)
    nu.bar      = nrow(Y) + priors$nu.prior
    S.bar       = priors$S.prior + t(Y)%*%Y + t(priors$A.prior)%*%diag(1/diag(kappa.posterior[s]*priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
    
    Sigma.posterior.dist   = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.draw             = apply(Sigma.posterior.dist,3,solve)
    Sigma.posterior[,,s]   = Sigma.draw
    A.posterior[,,s]            = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))
    L                      = t(chol(V.bar))
    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    
    #full conditional posterior of kappa
    if (s!=sum(S)){
    s.kappa.bar           = priors$s.kappa.prior + sum(diag(solve( Sigma.posterior[,,s])*t(A.posterior[,,s]-priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%(A.posterior[,,s]-priors$A.prior)))
    nu.kappa.bar          = priors$nu.kappa.prior + (K*N)
    kappa.draw            = s.kappa.bar/rchisq(1, df=nu.kappa.bar)
    kappa.posterior[s+1]  = kappa.draw
    }
  }
  
  posterior.extension = list(
    Sigma.posterior   = Sigma.posterior[,,S[1]+1:S[2]], #getting rid of first S[1] draws
    A.posterior       = A.posterior[,,S[1]+1:S[2]],
    kappa.posterior   = kappa.posterior[S[1]+1:S[2],1]
  )
  return(posterior.extension)
}

## Apply function BVAR_extension
posterior.draws = BVAR_extension(Y=Y, X=X, priors=priors, initial_kappa=initial_kappa, S=S)

```

The output of the BVAR function applied on the baseline model is: 
```{r output extension}
#| echo: true
#| message: false
#| warning: false

round(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)
round(apply(posterior.draws$A.posterior, 1:2, mean),3)
round(mean(posterior.draws$kappa.posterior),3)
```

# Forecasting

The objective of this report is to forecast the Household final consumption expenditures for the next 5 years (h=20). In this section, we will first base the forecasts on the baseline model and then repeat the same steps for the extended model where we estimate the hyper parameter $\kappa$ and the heteroskedasticity model. 

## Forecasting with the baseline model

To build the point forecasts, we use numerical integration to sample from the joint predictive density in the following steps: 
\01. We sample posterior draws from $p(A,\Sigma \mid Y,X)$ by using the BVAR estimation sampler created above. 
\02. We obtain $ \left\{ A^{(s)},\Sigma^{(s)} \right\}^S_{s=1}$
\03. We sample draws from $\hat{p}(Y_{t+h}\mid Y_t)$ by:
```{=tex}
\begin{align}
Y^{(s)}_{t+h} \sim \mathcal{N}_{hN}(Y_{t+h\mid t}(A^{(s)}), Var[Y_{t+h \mid t} \mid A^{(s)}, \Sigma ^{(s)}]))
\end{align}
```
\04. We obtain $\left\{Y^{(s)}_{t+h}\right\}^{S}_{s=1}$
\05. Characterise of the predictive density using $\left\{Y^{(s)}_{t+h}\right\}^{S}_{s=1}$

```{r forecasting the baseline model}
#| echo: true
#| message: false
#| warning: false
# simulate draws from the predictive density
library(mvtnorm)
h = 20
S = 50000
Y.h         = array(NA,c(h,N,S))

for (s in 1:S){
  x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]
  x.Ti        = x.Ti[p:1,]
  for (i in 1:h){
    x.T         = c(1,as.vector(t(x.Ti)))
    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%posterior.draws$A.posterior[,,s], sigma=posterior.draws$Sigma.posterior[,,s])
    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])
  }
}

# plots of forecasts
library(plot3D)
library(MASS)
library(HDInterval)

pce.point.f    = apply(Y.h[,1,],1,mean) #one pce forecasts
pce.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)
pce.range      = range(y[,1],pce.interval.f)

blue  = "#05386B"
plum      ="#BEBADA"
plum.rgb = col2rgb("thistle")
shade = rgb(plum.rgb[1],plum.rgb[2],plum.rgb[3],maxColorValue=255,alpha=100, names="thistle")



par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,1])+h),c(y[,1],pce.point.f), type="l", ylim=pce.range, axes=FALSE, xlab="", ylab="", lwd=2, col=plum)
axis(1,c(1,41,80,88, nrow(y),nrow(y)+h),c("2000","2010","2020","2022","",""), col=blue)
axis(2,c(pce.range[1],mean(pce.range),pce.range[2]),c("","HF Consumption Expenditures",""), col=blue)
abline(v=92, col="gray42")
text(x=91, y=12.7, srt=90, "2022 Q4")
abline(v=96, col="gray42")
text(x=95, y=12.7, srt=90, "2023 Q4")
polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[92,1],pce.interval.f[1,],pce.interval.f[2,20:1],y[92,1]),
        col=shade, border=plum)
dev.off()

```

The forecasts are built using the predictive density means and 90% highest density intervals. The above plot presents a sharp decline of personal consumption expenditures in the first quarters of 2023. A potential explanation could be that this drop is mainly driven by the decline in the data caused by the COVID-19 crisis. The personal consumption expenditures is then predicted to increase back in 2024. 

## Forecasting with the extended model 

# The model proof

## Set up of the model proof

```{r model proof set up}
#| echo: true
#| message: false
#| warning: false

### Specify the setup
p = 1
N = 2
K = 1+N*p
S = c(5000,50000)

### Generate RW data process
rw.1    = cumsum(rnorm(1000,0,1))
rw.2    = cumsum(rnorm(1000,0,1))
y       = matrix(cbind(rw.1,rw.2),nrow=1000,ncol=N)

### Create Y and X matrices
Y       = ts(y[2:nrow(y),])
X       = matrix(1,nrow(Y),1)
X       = cbind(X,y[1:nrow(y)-p,])

```

## Proof of baseline model

```{r proof of baseline}
#| echo: true
#| message: false
#| warning: false
### MLE
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

### Specify the priors (Minnesota prior)
kappa.1           = 0.02^2
kappa.2           = 100
A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)

priors = list(
  A.prior     = A.prior,
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),
  S.prior     = diag(diag(Sigma.hat)),
  nu.prior    = N+1 
)

## Apply function BVAR
posterior.draws = BVAR(Y=Y, X=X, priors=priors, S=S)
round(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)
round(apply(posterior.draws$A.posterior, 1:2, mean),3)
```
## Proof of extended model

```{r proof of extension}
#| echo: true
#| message: false
#| warning: false
### Proof of extension model

### MLE
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

### Modify the priors
kappa.1           = 1
kappa.2           = 10
initial_kappa     = 100
A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)

priors = list(
  A.prior            = A.prior,
  V.prior            = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),
  S.prior            = diag(diag(Sigma.hat)),
  nu.prior           = N+1,
  s.kappa.prior      = 2,
  nu.kappa.prior     = 4
)

## Apply function BVAR_extension
posterior.draws = BVAR_extension(Y=Y, X=X, priors=priors, initial_kappa=initial_kappa, S=S)
round(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)
round(apply(posterior.draws$A.posterior, 1:2, mean),3)
```

extendend? 
## References

Aprigliano, V., Ardizzi, G., & Monteforte, L. (2019), "Using Payment System Data to Forecast Economic Activity," International Journal of Central Banking, International Journal of Central Banking, vol. 15(4), pages 55-80, October.

Baker, S. R., Bloom, N., & Davis, S. J. (2016). Measuring Economic Policy Uncertainty. The Quarterly Journal of Economics, 131(4), 1593--1636. https://doi.org/10.1093/qje/qjw024

Carlsen, M. & Storgaard, P. E. (2010), "Dankort Payments as a Timely Indicator of Retail Sales in Denmark." Danmarks Nationalbank Working Papers n°66.

Ellingsen, J., Larsen, V. H., & Thorsrud, L. A. (2021). News media versus FRED‐MD for macroeconomic forecasting. Journal of Applied Econometrics, 37(1), 63 -- 81. https://doi.org/10.1002/jae.2859.

Esteves, P. S. (2009), "Are ATM/POS Data Relevant When Nowcasting Private Consumption?", Working Papers n°25, Banco de Portugal.

Galbraith, J. W., & Tkacz, G. (2013). Nowcasting GDP: Electronic Payments, Data Vintages and the Timing of Data Releases. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2342228.

Gil, M., Perez, J. J., Sanchez Fuentes, A. J., & Urtasun, A. (2018). "Nowcasting Private Consumption: Traditional Indicators, Uncertainty Measures, Credit Cards and Some Internet Data", Working Paper No. 1842, Banco de Espana.

Lazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). The Parable of Google Flu: Traps in Big Data Analysis. Science (New York, N.Y.), 343, 1203--1205. https://doi.org/10.1126/science.1248506

Schmidt, T., & Vosen, S. (2009). Forecasting Private Consumption: Survey-Based Indicators vs. Google Trends. Journal of Forecasting, 30. https://doi.org/10.2139/ssrn.1514369

:::
